{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neuroimager's Guide to the CHBH","text":"<p>Welcome to the Neuroimager's Guide!</p> <p>These pages contain practical information on data collection and analysis at the CHBH. We also extend the BEAR Technical Documentation pages to provide tutorials on neuroimaging quality control and data analyses using the University of Birmingham's Supercomputer - BlueBEAR.</p>"},{"location":"#click-a-modality-to-get-started","title":"Click a modality to get started","text":"<ul> <li>MRI add image? </li> <li>MEG add image? </li> <li>EEG add image? </li> <li>Sleep add image? </li> <li>Behaviour add image? </li> <li>Brain Stimulation add image? </li> </ul> <p>Note</p> <p>This website is intended for current staff and students at the Centre for Human Brain Health and University of Birmingham. It may or may not be useful for anyone else...</p>"},{"location":"#contributors","title":"Contributors","text":"<p>Many thanks to our contributors!</p> <sub>ajquinn</sub>\ud83d\udea7 \ud83d\udd8b <sub>James Carpenter</sub>\ud83d\udea7 \ud83d\udd8b <sub>Brandon Ingram</sub>\ud83d\udd8b <sub>Arkady Konovalov</sub>\ud83d\udd8b <sub>Ben Griffiths</sub>\ud83d\udd8b <sub>Tara</sub>\ud83d\udd8b <sub>Dagmar S Fraser</sub>\ud83d\udd8b <sub>Jianzhang Ni</sub>\ud83d\udd8b <sub>Katharina</sub>\ud83d\udd8b <sub>Tommy Roberts</sub>\ud83d\udd8b <sub>Aamir Sohail</sub>\ud83d\udea7 \ud83d\udd8b <sub>Selma Lugtmeijer</sub>\ud83d\udd8b <sub>TomRhysMarshall</sub>\ud83d\udd8b <sub>Nicholas Paul Holmes</sub>\ud83d\udd8b"},{"location":"#contributing","title":"Contributing","text":"<p>This page is a work-in-progress and contributions are very welcome! Please email Andrew or make some changes directly on the CHBH-on-BEAR GitHub page. See the 'Making a contribution' page for more information on how to do so.</p>"},{"location":"faqs/","title":"Frequently Asked Questions","text":"<p>Is something incorrect, out-of-date or missing from this page? Open an issue on github and let us know.</p>"},{"location":"faqs/#where-can-i-ask-for-help","title":"Where can I ask for help?","text":"<p>You can ask for informal help via the CHBH Code, Computing and Open Science teams page. More formal support is availabl by raising a support ticket with IT.</p>"},{"location":"faqs/#general","title":"General","text":"<p>What is a 'core'? is it the same as a 'CPU'?</p> <p>Core and CPU refer to a Central Processing Unit. The terms are sometimes used interchangeably.</p> <p>What does it mean if I use '4 cores'?</p> <p>Each core can run a single computation at a time. If we have 4 cores, we can run 4 different things can happen at once. And 50 cores means 50 jobs could be run in parallel.</p> <p> Is castles the same as rds space?</p> <p>CaStLeS (Compute and Storage for Life Sciences) resources (both compute and storage) are a constituent part of BEAR Cloud, BlueBEAR and the wider BEAR infrastructure. They are reserved exclusively for the use of research groups carrying out research in the life sciences.</p> <p>In CHBH, 'Castles' has often referred to Virtual Machine environments used for analyses. These are been phased out in favour of the BEAR GUI.</p> <p> What about code sharing? Not everybody has a Bluebear. Will people be able to run my code if I\u2019ve written it for use on this platform?</p> <ul> <li>Think in terms of \u2018develop\u2019 vs \u2018rationalise\u2019. The second part is where parallelising becomes a bi advantage. It\u2019s also the place you think about code sharing.</li> <li>There are clever ways to make things very shareable agnostic to infrastructure. E.g., putting everything into a docker container and running \u2018container jobs\u2019. This has the disadvantage that you lose bear\u2019s optimisation (all modules have been optimised so may run a lot faster). But you gain control. Tradeoff</li> </ul>"},{"location":"faqs/#bear-portal","title":"BEAR Portal","text":"<p>What is included in the 'Number of hours' when starting a service on BEAR portal?</p> <p>The \u2018number of hours\u2019 means that the GUI is required for N hours, not that the whole analysis needs to last N hours.  The 8 hour limit is for interactive jobs or apps started from the BEAR portal. The time limit on the main cluster is 10 days.</p> <p>What does \u2018working directory\u2019 mean when starting VS Code?</p> <p>This changes root dir that VScode opens in, though it can only operate within your RDS home space, not a group space.</p> <p>If you want to edit files in a group space using VS Code, you'll need to create a 'Symbolic Link' that provides a path to the group from within your home space. For example, to link a project to a home space you can run</p> <pre><code>ln -s /rds/groups/q/quinna-example-project /rds/homes/q/quinna/\n</code></pre> <p>There will now be a link directory in <code>/rds/homes/q/quinna/quinna-example-project</code> that you can use to see the files in the group space. Note that this is a 'link' not a copy.</p> <p>Do I NEED to run a VScode interactive session - couldn\u2019t I use my own local code editor?</p> <p>If you\u2019ve mounted your RDS drive, you could also use a local text editor, any one that you like. But, if you\u2019re running things locally on BEAR, an advantage is that you don\u2019t need to copy files back and forth which adds a step and can be cumbersome.</p> <p>Does VScode track changes to code?</p> <p>VScode doesn\u2019t track changes itself but VScode should be able to show you which files have been modified if you are reading files within an existing git repository. Git should be used to handle version tracking.</p>"},{"location":"faqs/#matlab","title":"Matlab","text":"<p>What situations would it make sense to use parfor in matlab?</p> <p>The most common neuroimaging use-case for <code>parfor</code> is looping over participants for first-level analyses. For example, when you want to run an identical set of steps on every file in a datasets.</p> <p>Other examples might include computing something from every voxel/channel/frequency in a single dataset, or running non-parametric permutatation stats.</p> <p>What if you asked for 50 CPUs and DIDN\u2019T use a parfor? Would it still be faster?</p> <p>Not necessarily. It depends on how the person wrote the code, occasionally a toolbox might know how to parallelise a process internally but this is unlikely by default. It is possible to ask for 50 cores and then accidentally run an analysis on only one of them.w</p> <p>Can I write a startup.m script to do things like adding paths to tools I use all the time, so MATLAB will automatically execute this when it starts up?</p> <p>Yes. A matlab <code>startup.m</code> script in your local home directory will run as normal when opening matlab.</p> <p>For people who are already using matlab/rdesktop on their own computer, what are the key steps to get the best optimisation as you\u2019ve described when they switch to the cluster?</p> <p>Don\u2019t optimise too early. Look at your data first - lots of visualisation etc which is more interactive. But when you\u2019ve got your pipeline hardened, that\u2019s when you want to do all this stuff. Or when you\u2019ve got a pipeline but want to change something and re-run.</p>"},{"location":"getting_started/","title":"Getting started on BEAR","text":"<p>This page collects tutorials and examples for neuroimaging analyses to run on BlueBEAR. This is intended to extend the main BEAR Technical Documentation pages.</p> <p>Here we provide a set of links, tips and tricks for getting started.</p>"},{"location":"getting_started/#step-0-linux","title":"Step 0: Linux","text":"<p>BEAR provide a Introduction to Linux guide. Many computing services on BEAR rely on Linux. There are in-person workshops and an online Canvas courses available on this page.</p>"},{"location":"getting_started/#step-1-bluebear","title":"Step 1: BlueBEAR","text":"<p>BEAR also provide a Introduction to BlueBEAR course. There are in person workshops and an online Canvas course.</p>"},{"location":"getting_started/#step-2-rds-projects","title":"Step 2: RDS Projects","text":"<p>You'll need to be a member of a BEAR project and have a BEAR linux account to use BlueBEAR. Your PI and lab can help with this. A detailed guide for accessing BEAR is provided on the technical docs.</p>"},{"location":"getting_started/#step-3-bear-portal","title":"Step 3: BEAR Portal","text":"<p>BEAR Portal provides web-based access to a range of BEAR services, such as JupyterLab, RStudio, and various other GUI applications. BEAR Portal is only available on campus or using the University Remote Access Service.</p>"},{"location":"getting_started/#step-4-launching-interactive-sessions","title":"Step 4: Launching interactive sessions","text":"<p>From the BEAR Portal there are three options for launching an interactive analysis session.</p> <ul> <li> <p>Some software packages have GUI Apps installed on BlueBEAR that can be launched from the BEAR Portal - the main example for neuroimaging analysis is MATLAB.</p> </li> <li> <p>JupyterLab and RStudio are installed as standalone apps that can be launched from the BEAR Portal (note that only packages installed on BEAR Apps are available to load in JupyterLab).</p> </li> <li> <p>A complete Linux Desktop can be launched as the BlueBEAR GUI. BlueBEAR GUI is effectively a blank-slate Linux desktop, into which you can load the modules for various applications, specify environment variables etc. by using the built-in Terminal client, and then ultimately launch the interface for the application that you require.</p> </li> </ul>"},{"location":"getting_started/#step-5-running-cluster-jobs-with-slurm","title":"Step 5: Running cluster jobs with Slurm","text":"<p>There are two ways to submit a cluster job - the BlueBEAR terminal and the job submission page on BEAR Portal</p> <p>BlueBEAR Terminal Once you have prepared a submission script, you can go to that location in the BEAR Portal file browser and click 'Open in terminal' in the top. This will open a terminal session from which you can submit, monitor and (optionally) cancel your job using <code>sbatch</code>, <code>squeue</code> and <code>scontrol</code>.</p> <p>Note</p> <p>These terminal sessions are ONLY intended for submitting and monitoring cluster jobs - not for active analyses. This should be carried out on the BEAR GUI or similar.</p> <p>BlueBEAR Job Composer this is a GUI page which helps you to write a new job to submit to BlueBEAR using the job composer.</p>"},{"location":"making_a_contribution/","title":"Making a contribution","text":"<p>This page will teach you how to add contributions to the chbh-on-bear GitHub page. This page outlines the whole process, from installing Git to requesting your contribution to be added. We assume no knowledge of Git or GitHub.</p>"},{"location":"making_a_contribution/#installing-git","title":"Installing Git","text":""},{"location":"making_a_contribution/#linux","title":"Linux","text":"<p>You can install Git through your package manager in the terminal, this will depend on your particular distro.</p> <p>For Debian : </p> <pre><code>$ sudo apt install git-all\n</code></pre> <p>For Fedora :</p> <pre><code>$ sudo dnf install git-all\n</code></pre> <p>For other distros see other distro installs.</p>"},{"location":"making_a_contribution/#windows","title":"Windows","text":"<p>You can install git from a windows installer or by using a package manager similar to linux.</p> <p>Using Winget :</p> <pre><code>&gt; winget install -e --id Git.Git\n</code></pre>"},{"location":"making_a_contribution/#making-a-github-account","title":"Making a Github account","text":"<p>You will also need to have a GitHub account to be able to make a change request. Make an account on GitHub</p> <p>After making an account configure your git to use that account within your terminal.</p> <pre><code>$ git config --global user.email \"Add GitHub Email address here within quotes\"\n$ git config --global user.name \"Add GitHub Username here within quotes\"\n</code></pre> <p>You may also need to generate a personal access token :</p> <ul> <li>On GitHub click your profile picture</li> <li>Settings</li> <li>Developer Settings</li> <li>Personal Access Tokens </li> <li>Tokens (Classic)</li> <li>Generate New Token</li> <li>Repo</li> <li>Generate New Token</li> <li>Place this token somewhere safe this will be used for your password</li> </ul>"},{"location":"making_a_contribution/#working-with-git-locally","title":"Working with Git Locally","text":""},{"location":"making_a_contribution/#creating-a-branch","title":"Creating a Branch","text":"<p>A branch is a version of the project that is different from the main, this is made so that changes do not effect the main project.</p> <p>Check what branch you are on :</p> <pre><code>$ git branch -l\n</code></pre> <p>Create your own branch :</p> <pre><code>$ git checkout -b tommy_changes\n</code></pre> <p>This will create a new branch called \"tommy_changes\" and switch to it. You can check what branch you are in again to make sure you have switched. </p>"},{"location":"making_a_contribution/#making-changes","title":"Making changes","text":"<p>Once you are on your own branch you can start making changes. To make changes add or edit the markdown files in the \"Docs\" folder, this can be done in your usual text editor. If you are unfamiliar with markdown check out this guide If you have created a new markdown file then this can be added to the website in the \"mkdocs.yml\" file.</p> <p>Adding the page to mkdocs.yml : </p> <pre><code>nav:\n  - Python: software/python.md\n</code></pre>"},{"location":"making_a_contribution/#adding-your-changes","title":"Adding your changes","text":"<p>To check the status of you changes use : </p> <pre><code>$ git status\n</code></pre> <p>Use this whenever you're unsure as what is happening with your file changes.</p> <p>If you've created a new file add it to git using it's filename.</p> <p>Example file add :</p> <pre><code>$ git add docs/software/python.md\n</code></pre> <p>To see the differences you've made you can use git in your terminal.</p> <pre><code>$ git diff\n</code></pre> <p>Once you've made all the changes you would like to make, you can then save all these changes to your git branch. As good practice these changes should be similar in nature such as \"Correcting spelling errors\". </p> <p>Saving your changes to your branch :</p> <p><pre><code>$ git commit -m \"Correcting spelling errors\"\n</code></pre> This will save all your changes with the messsage \"Correcting spelling error\". This will be visable publically later and be used to explain your changes to others. </p>"},{"location":"making_a_contribution/#working-with-github","title":"Working with GitHub","text":""},{"location":"making_a_contribution/#pushing-those-changes-to-public-branch","title":"Pushing those changes to public branch","text":"<p>To put our changes on GitHub for the project so everyone can see. We must first make a public branch similar to how we created our local branch before. You will be prompted to enter your username for GitHub, use the username we set up before. It will then ask for a password, do not use your GitHub password, use your git token we generated before (Note when you type or copy this token in, it will not be shown in the terminal but it is registering inputs. This can be odd for users unfamiliar.). </p> <p>Creating GitHub branch : </p> <pre><code>$ git push --set-upstream origin tommy_changes\nUsername for 'https://github.com': TommyTeapot\nPassword for 'https://github.com': {Your token here}\n</code></pre> <p>To then push those changes to this new branch use : </p> <pre><code>$ git push origin tommy_changes\nUsername for 'https://github.com': TommyTeapot\nPassword for 'https://github.com': {Your token here}\n</code></pre> <p>This will then push these changes to the public branch that is viewable for everyone. If you wish to push another commit you can skip the creating a branch section.</p>"},{"location":"making_a_contribution/#making-a-request-for-your-contribution","title":"Making a request for your contribution","text":"<p>After doing all these previous steps the final thing you need to do is create a pull request. This lets the admins know that you want something merged into the main branch that will then be put automatically onto the chbh-on-bear page. </p> <ul> <li>In your browser go onto the chbh-on-bear GitHub page</li> <li>Then on branches select the branch that you have created and made your commits to</li> <li>Click \"Compare and pull request\"</li> <li>Type the title and description of your pull request</li> <li>Check will be run automatically on your pull request and then will be reviewed by the admin team</li> </ul> <p>If review is sucessesful your changes will be merged into the main branch. Otherwise there is a section to have a conversation about those changes and add more commits. Feel free to make any changes you feel are valuable and thank you for contributing!</p>"},{"location":"making_a_contribution/#changes-to-the-main-branch","title":"Changes to the main branch","text":"<p>When working on this project and making multiple commits it's possible the main branch will be updated. It's important to work on a version that is close to the live version to avoid conflicts. </p> <p>Move onto main branch : </p> <pre><code>$ git checkout main\n</code></pre> <p>Check you are on the main branch : </p> <pre><code>$ git branch\n* main\n</code></pre> <p>Get updated main branch :</p> <pre><code>$ git fetch origin \n</code></pre> <p>Pull the update into local main : </p> <pre><code>$ git pull origin main\n</code></pre> <p>Move back to your branch : </p> <pre><code>$ git checkout tommy_changes\nSwitched to branch tommy_changes\n</code></pre> <p>Check you are on your branch :</p> <pre><code>$ git branch\n* tommy_changes\n</code></pre> <p>Merge updated main into your branch : </p> <pre><code>$ git merge main\n</code></pre> <p>Update the public GitHub branch :</p> <pre><code>$ git push origin tommy_changes\n</code></pre> <p>This will update you local main and personal branch to be up to date with the current live version and then push that to the public version of the branch. Editing is done exactly the same as before, happy contributing! </p>"},{"location":"behaviour/","title":"Behavioural Testing at CHBH","text":"<p>Guides and documentation for behavioural data collection at the CHBH.</p> <ul> <li> <p>Technical Information</p> <p>Essential hardware and infrastructure details</p> <ul> <li>Cubicles</li> <li>Common Experimental Platform PCs</li> <li>Troubleshooting</li> </ul> </li> <li> <p>Eye Tracking</p> <p>Tracking eyes and pupilometry</p> </li> </ul> <p>CHBH Sharepoint</p> <p>These pages contains public facing information about behavioural testing at CHBH. For details on projects codes, approvals, ethics, finances, bookings and administration please see the CHBH Sharepoint pages (UoB SSO login required)</p>"},{"location":"behaviour/technical/cep/","title":"Common Experimental Platform PCs","text":"<p>Currently the Brain Stim Lab, the fMRI, the Mock, the MEGs, fNIRS, the OPM and some Cubicles have identical stimulus presentation / response recovery PCs built specifically for research grade timing, durability and redundancy.</p> <p>We intend to install such CEP (common experimental platform) Stimulus machines in all suitable modalities. This will encourage paradigm mobility between the modalities (write once, run anywhere), and allow enhanced response from CHBH IT personnel.</p>"},{"location":"behaviour/technical/cep/#the-cep-pc","title":"The CEP PC","text":"<p>Note</p> <p>Information on log-ins, network names and administration of CEP PCs is covered on the CHBH sharepoint pages which require UoB SSO log in.</p>"},{"location":"behaviour/technical/cep/#hardware","title":"Hardware","text":"<p>High end Intel CPUs and 16GB of RAM on ASUS PRIME Z370-A II (or it's predecessor Z270-A) motherboards. Graphics are provided by AMD WX (4100 or 7100) series of workstation graphics cards, as specifically recommended by the author of the PsychoPhysics Toolbox (PTB) under Neurodebian.</p> <p>AMD WX 4100 &amp; WX7100 graphics cards. The WX7100 cards will be in the MEG (in transition from NVIDIA), MOCK and fMRI. 4100s in the cubicles and other modalities. (2023 will see new PCs equipped with the newer W7500 cards)</p> <p>Onboard audio is the recommended solution for the PTB, replacing the previous ASIO recommendation (which has been fully retired in recent version of PTB).</p> <p>AOC 24\" monitor 1920x1080@120Hz (to match the standard operation of the VPIxx projector in the MEG and fMRI chambers). Check that desktop refresh rate is set to 120Hz and that AMD FreeSync is disabled - otherwise the PTB will fail the SimpleMovieDemo test. Some PCs will now be equipped with 360Hz monitors in the EEG lab.</p> <p>Single monitors are indicated for precise timing control in the PTB. In Labs where multiple monitors are indicated, experimenters should use a hardware splitter and not a second output port on the AMD WX card (especially under Windows 10). The VPIXX Projector provides it's own pass through that preserves timing control.</p>"},{"location":"behaviour/technical/cep/#software","title":"Software","text":"<p>Generally Windows 10; MATLAB 2018a and 2019b, with the most recent PTB. PsychoPy 3.0.3. National Instruments DAQmx. LABJACK U3 package. Every machine should have an identical OS installation. We will be updating the MATLAB versions in the beginning of 2024.</p> <p>To provide a consistent experience. MATLAB will clean it's path of any non basic paths on restart. Please add suitable code to the beginning of your experiment, such as addpath(genpath(\u2018yourfolder)); . As this is a multi-user machine this allows us to avoid namespace clashes for inbuilt and bespoke scripts.</p> <p>All CEPs are capable of running Neurodebian 18 LTS; MATLAB 2018a, with the most recent PTB. This is the preferred platform for precise timing.</p> <p>See this article, from the author of PsychoPy, for a discussion of timing reliability betwixt PsychoPy/PTB and Windows/Linux.</p> <p>Some Labs may request that their CEP not be constantly connected to the Network.</p>"},{"location":"behaviour/technical/cep/#paradigm-location-windows","title":"Paradigm Location (Windows)","text":"<p>The C: boot drive is not for the storage of responses or paradigms. The C: boot drive is not backed up. The C: boot drive can and will be re-imaged at any time. A remote drive R: Remote Paradigms is provided to offer a shared repository of paradigms and your data. This will allow access from any other CEP PC. This is backed up. - How to Map the Remote Paradigms Drive. A second drive L: LOCAL is provided for paradigms to be hosted locally if your paradigm is impacted running from a network resource. This is not backed up.</p>"},{"location":"behaviour/technical/cubicles/","title":"Behavioural testing cubicles","text":""},{"location":"brainstim/","title":"Brain Stimulation at CHBH","text":"<p>Guidelines and documentation for all Brain Stimulation methods at CHBH</p> <ul> <li> <p>Lab overview</p> <p>Information about the CHBH brain stimulation lab</p> <ul> <li>Lab overview</li> </ul> </li> <li> <p>Focused Ultrasound Stimulation (FUS)</p> <p>Everything FUS-related, including pre-sonication planning</p> <ul> <li>Equipment Overview</li> <li>Planning with k-Plan</li> <li>Transducer Calibration</li> </ul> </li> </ul>"},{"location":"brainstim/tms/","title":"Transcranial magnetic stimulation (TMS)","text":"<p>TMS can be used to measure brain activity levels (e.g., using EMG for studies of the motor system), to interfere with brain activity (e.g., in perceptual or cognitive studies), or to induce changes in the brain (e.g., in clinical interventions). A TMS study at minimum needs a stimulator and a coil. TMS can be used in conjunction with other stimulation and recording modalities such as EMG, eye-tracking, MRI or EEG.</p>"},{"location":"brainstim/tms/#guidance","title":"Guidance","text":"<p>The following international consensus statements and guidance are required reading for all new TMS researchers.</p> <p>Rossini et al. 2015: Non-invasive electrical and magnetic stimulation of the brain, spinal cord, roots and peripheral nerves: basic principles and procedures for routine clinical and research application.</p> <p>Rossi et al. 2009: Safety and recommendations for TMS use in healthy subjects and patient populations, with updates on training, ethical and regulatory issues: expert guidelines</p>"},{"location":"brainstim/tms/#hardware","title":"Hardware","text":""},{"location":"brainstim/tms/#stimulator","title":"Stimulator","text":"<p>Magstim Rapid [model number]</p> <p>![image_of_TMS_here]</p> <p>The Rapid delivers biphasic pulses at up to [max] Hz. The intensity of stimulation will detemine how long each train of stimulation can be applied for.</p>"},{"location":"brainstim/tms/#coils","title":"Coils","text":"<ul> <li>[diameter] mm figure-of-eight [model number]</li> <li>[diameter] mm figure-of-eight [model number]</li> </ul> <p>![image_of_TMS_coils_here]</p>"},{"location":"brainstim/tms/#interface-devices","title":"Interface devices","text":"<p>[D-type x pins] digital input-output port.</p> <p>![image_of_back_of_TMS_here]</p>"},{"location":"brainstim/tms/#electromyography","title":"Electromyography","text":"<ul> <li>[do we have this?] 1401?</li> </ul>"},{"location":"brainstim/tms/#neuronavigation","title":"Neuronavigation","text":"<p>Some studies (e.g., on the motor system) may not use neuronavigation to locate the stimulation target, because they can use body movement or EMG instead. For targeting other brain areas, neuronavigation may be important. For further details, see the main neuronavigation page.</p>"},{"location":"brainstim/tms/#software","title":"Software","text":"<p>TMS studies can be run in stand-alone mode, using only the software interface provided with the stimulator. Alternatively, the TMS can be controlled by an external device using 5V TTL pulses or a digital interface.</p>"},{"location":"brainstim/tms/#tmsmultilab","title":"TMSMultiLab","text":"<p>Set up by members of the University of Birmingham, TMSMultiLab is an international community of researchers using TMS that shares guidance, resources, code &amp; data for TMS research. New members are always welcome. Contact Nick Holmes for more details.</p>"},{"location":"brainstim/FUS/calib/","title":"Equipment Calibration","text":"<p>PLACEHOLDER</p> <p>Include SOP for transducer calibration in water bath etc.</p>"},{"location":"brainstim/FUS/equip/","title":"FUS Equipment","text":"<p>The following equipement is part of the 'standard' FUS setup at CHBH:</p> <ul> <li> <p>NeuroFUS PRO Transducer Power Output (TPO). Upgraded to TPO-105, including the following features:</p> <ul> <li>Up to 400W electrical power</li> <li>Up to 100W per channel</li> </ul> </li> <li> <p>NeuroFUS Transducers (4):</p> <ul> <li>CTX-500-4CH 500kHz, 4 element</li> <li>CTX-250-4CH 250kHz, 4 element</li> <li>CTX-500-2CH 500kHz, 2 element</li> <li>DPX-500-4CH 'Deep Transducer', 500kHz, 4 element</li> </ul> </li> <li> <p>Transducer Verification Kit</p> <ul> <li>Hydrophone</li> <li>Test container with transducer and hydrophone holder</li> <li>Multimeter</li> </ul> </li> <li> <p>k-Plan Software and software licenses</p> </li> </ul> <p>We additionally have the following equipment for deploying FUS in the MRI scanner (not currently in active use)</p> <ul> <li>NeuroFUS MRI-compatible Transducer and Cable upgrade</li> <li>BrainSight MRI Camera Stand</li> <li>BrainSight MRI-compatible trackers (for volunteer and transducers)</li> </ul>"},{"location":"brainstim/FUS/kplan/","title":"Pre-Sonication Planning with k-Plan","text":"<p>This page will eventually include a walkthrough for using k-Plan to perform pre-sonication planning.</p>"},{"location":"eeg/","title":"EEG at CHBH","text":"<p>Guides and documentation for EEG analysis and data collection.</p> <ul> <li> <p>Technical Information</p> <p>Essential hardware and infrastructure details</p> <ul> <li>The EEG Devices</li> </ul> </li> <li> <p>Data Acquisition</p> <p>Equipment and setup for data collection</p> </li> <li> <p>Stimulus Delivery</p> <p>Software and methods for stimulus presentation</p> </li> <li> <p>Quality Control</p> <p>Analysis and data quality procedures</p> <ul> <li>FieldTrip</li> </ul> </li> </ul> <p>CHBH Sharepoint</p> <p>These pages contains public facing information about EEG at CHBH. For details on projects codes, approvals, ethics, finances, bookings and administration please see the CHBH Sharepoint pages (UoB SSO login required)</p>"},{"location":"eeg/analysis/fieldtrip/","title":"Fieldtrip on Slurm","text":"<p>Note</p> <p>Example contributed by Ben Griffiths.</p> <p>This is an example script running a fieldtrip analysis on EEG data acqurired during a visual flicker task.</p> <p>The data is read in, filtered, epoched, ICA'd, re-referenced, then plotted. The core function can be executed on the MATLAB GUI App during an interactive session, or submitted to BlueBEAR using the <code>bash</code> script below.</p>"},{"location":"eeg/analysis/fieldtrip/#core-processing-script","title":"Core processing script","text":"<p>The following code can be saved as <code>basic_preprocessing.m</code>.</p> <pre><code>%% Basic Preprocessing\n% A script to demonstrate how one can (superficially) preprocessing EEG\n% data using Fieldtrip, Matlab and BlueBEAR.\n%\n% Benjamin J. Griffiths (b.griffiths.1 [at] bham.ac.uk)\n% 28th March 2023\n\n%% Prepare Workspace\n% define root directory where data is stored\nroot_dir = '/rds/projects/g/griffibz-example-project/msc-eeg-23/';\n\n% add fieldtrip to path\naddpath('/rds/projects/g/griffibz-example-project/fieldtrip/')\nft_defaults\n\n% define participant number\nsubj = 1;\n\n%% Filter Raw Data\n% load data\ncfg         = [];\ncfg.dataset = sprintf('%s/bids/sub-%02.0f/eeg/sub-%02.0f_task-eeg-flicker_eeg.eeg', root_dir, subj, subj); % dynamically determine dataset name\ndata        = ft_preprocessing(cfg);\n\n% remove external and trigger channels\ncfg         = [];\ncfg.channel = {'all', '-EX*', '-Status'}; % select all channels except any external (-EX*) or trigger (-Status) channel\ndata        = ft_selectdata(cfg, data);\n\n% filter data\ncfg             = [];\n%cfg.hpfilter    = 'yes';   % apply high-pass filter\n%cfg.hpfreq      = 0.8;     % use high-pass to suppress frequencies &lt; 0.8Hz\ncfg.lpfilter    = 'yes';   % apply low-pass filter\ncfg.lpfreq      = 120;     % use low-pass to suppress frequencies &gt; 120Hz\ncfg.bsfilter    = 'yes';   % apply band-pass filter\ncfg.bsfreq      = [49 51]; % use band-pass to suppress frequencies netween 49Hz and 51Hz\ndata            = ft_preprocessing(cfg, data);\n\n%% Epoch Data\n% load in BIDS event file\nevents = readtable(sprintf('%s/bids/sub-%02.0f/eeg/sub-%02.0f_task-eeg-flicker_events.tsv', root_dir, subj, subj),'Filetype','text'); % dynamically determine dataset name\n\n% define Fieldtrip-style event structure\ntrl_start = -2; % start trial 2 seconds before trigger\ntrl_end = 4; % end trial 4 seconds after trigger\ntrl_def(:,1) = events.sample + (trl_start * data.fsample); % define samples to start trial\ntrl_def(:,2) = events.sample + (trl_end * data.fsample); % define samples to end trial\ntrl_def(:,3) = trl_start * data.fsample; % define when time = 0 occurs relative to start of trial\n\n% epoch data\ncfg = [];\ncfg.trl = trl_def;\ndata = ft_redefinetrial(cfg, data);\n\n% load in trialinfo\nload(sprintf('%s/bids/sourcedata/sub-%02.0f_trialinfo.mat', root_dir, subj))\ndata.trialinfo = trialinfo; % add trialinfo to data structure\n\n% tidy workspace\nclear events trl_start trl_end trl_def trialinfo\n\n%% Run ICA\n% restrict to retrieval trials\ncfg         = [];\ncfg.trials  = find(cellfun(@(x) strcmpi(x.trl_type, 'retrieval'), data.trialinfo));\ndata        = ft_selectdata(cfg, data);\n\n% reduce sample rate\ncfg = [];\ncfg.resamplefs = 256; % drop sample rate from 1024Hz to 256Hz\ndata = ft_resampledata(cfg, data);\n\n% run ica\nrng(subj) % set random seed to ensure reproducible outputs every time the function is run\nica = ft_componentanalysis([], data); % \"cfg\" need not be defined if using default settings\n\n% visualise first 20 components (commented to stop execution when running via Slurm)\n%ft_topoplotIC(struct('component',1:20,'layout','biosemi128.lay'), ica)\n\n% remove components\ncfg             = [];\ncfg.component   = [1 3]; % 1 = eyeblink, 3 = saccade\ndata            = ft_rejectcomponent(cfg, ica);\n\n%% Re-reference Data\n% re-reference to the average of all channels\ncfg = [];\ncfg.reref = 'yes';\ncfg.refchannel = 'all';\ndata = ft_preprocessing(cfg, data);\n\n%% Plot Results\n% get timelocked average of data\ncfg = [];\ncfg.channel = 'A*'; % restrict to posterior quadrant of channels\ntml = ft_timelockanalysis(cfg, data);\n\n% baseline correct timelocked average\ncfg = [];\ncfg.baseline = [-0.25 -0.05]; % set baseline as -250ms to -50ms\ntml = ft_timelockbaseline(cfg, tml);\n\n% plot ERP\nh = figure;\nsubplot(2,1,1); hold on\nplot(tml.time, mean(tml.avg))\nxlim([-0.5 2.5])\nxline(0,'k--')\nyline(0,'k-')\nxlabel('Time (s)')\nylabel('Amplitude (uV)')\ntitle('Visual Evoked Potential')\n\n% cycle through trials\npow = cell(8, 1); % create empty cells for eight conditions\nfor trl = 1 : numel(data.trial)\n    condition = data.trialinfo{trl}.ret_freq; % determine flicker condition\n    channels_A = cellfun(@(x) strncmpi(x, 'A', 1), data.label); % identify posterior channels\n    signal = data.trial{trl}(channels_A, :); % extract signal over posterior channels\n    pow{condition}(end+1,:) = mean(abs(fft(signal')')); % compute FFT\nend\n\n% determine frequencies of FFT\nfreqs = linspace(0, data.fsample, size(pow{1},2));\n\n% plot FFT for each condition\nsubplot(2,1,2); hold on\nfor condition = 1 : numel(pow)\n    plot(freqs,mean(pow{condition}));\nend\nxlim([6, 42])\nylim([0, 700])\ntitle('Power Spectrum')\nxlabel('Frequency (Hz)')\nylabel('Power (arb. units)')\nlegend({'60Hz','40Hz','30Hz','24Hz','20Hz','17.1Hz','15Hz','Baseline'})\n\n% save figure in root directory\nsaveas(h, sprintf('%s/basic_preproc_output.jpg', root_dir))\n</code></pre>"},{"location":"eeg/analysis/fieldtrip/#cluster-submit-script","title":"Cluster submit script","text":"<p>The following can be saved as a shell script and submitted to the cluster using <code>sbatch</code>.</p> <pre><code>#!/bin/bash\n\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --time 1:0:0\n#SBATCH --qos bbdefault\n#SBATCH --mail-type ALL\n\nset -e\n\nmodule purge; module load bluebear\nmodule load MATLAB/2021b\n\nmatlab -nodisplay -r \"basic_preprocessing; exit;\"\n</code></pre>"},{"location":"eeg/hardware/eeg/","title":"EEG at CHBH","text":"<p>EEG (electroencephalogram) is a non-invasive technique where electrodes are placed on a person's scalp to record electrical activity in the brain. It does this by recording changes in potential difference. EEG can capture hundreds of data points per second and therefore is a great tool for researching the chronology of mental processes. EEG is completely safe and pain free for participants.</p> <p>The CHBH has XXX BioSemi's Active Two housed in YYYY labs.</p>"},{"location":"meg/","title":"MEG at CHBH","text":"<p>Guides and documentation for MEG analysis and data collection at the CHBH.</p> <ul> <li> <p>Technical Information</p> <p>Essential hardware and infrastructure details</p> <ul> <li>The MEG</li> <li>MEG Safety</li> </ul> </li> <li> <p>Data Acquisition</p> <p>Equipment and setup for data collection</p> <ul> <li>MEG Acquisition Checklist</li> </ul> </li> <li> <p>Stimulus Delivery</p> <p>Software and methods for stimulus presentation</p> </li> <li> <p>Quality Control</p> <p>Analysis and data quality procedures</p> <ul> <li>MNE-Python</li> </ul> </li> </ul> <p>CHBH Sharepoint</p> <p>These pages contains public facing information about MEG at CHBH. For details on projects codes, approvals, ethics, finances, bookings and administration please see the CHBH Sharepoint pages (UoB SSO login required)</p>"},{"location":"meg/acquisition/meg-acquisition-checklist/","title":"MEG Data Acquisition Checklist","text":"<p>Click on the checklists to mark your progress through data collection.</p>"},{"location":"meg/acquisition/meg-acquisition-checklist/#prepare-control-room-msr","title":"Prepare Control Room &amp; MSR","text":"<ul> <li> <p> Check functioning of stimulus and response equipment.</p> Select the stim PCSelect the parallel port <ul> <li>Select the correct Stim PC via KVM.<ul> <li>Press button 1 to connect to the OLD Stim PC.</li> <li>Press button 3 to connect to the NEW Stim PC.<ul> <li>LED Meanings</li> <li>Green/Dark for connected PCs, but not active.</li> <li>Green/Red for connected PCs actively using the KVM.</li> <li>Dark/Dark for no connection.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Select the correct Parallel Port via the Parallel Port (PP) Switch Box.<ul> <li>Use PP Switch position A to connect STI101 to the OLD Stim PC (PP Base Memory Address: BFF8)</li> <li>Use PP Switch position B to connect STI101 to the NEW Stim PC (PP Base Memory Address: CFF8)</li> </ul> </li> </ul> </li> <li> <p> Check Gantry position. Move from Liquefaction (25) position to usage position - [Moving the Gantry]</p> </li> <li> <p> Check experimental paradigm.</p> Check the PROPixx projectorCheck Stimuli, Responses and Triggers <ul> <li>Check the PROPixx projector is \"awake\" (make sure lens cover is removed).<ul> <li>Start the VPutil program, from the Stim Desktop shortcut, and type...</li> <li>(ANY DEVICE) &gt; <code>ppx a</code> - should respond with \"PROPixx is in awake mode\"</li> <li>To \"sleep\" the projector ...</li> <li>(ANY DEVICE) &gt; <code>ppx s</code> - should respond with \"PROPixx is in sleep mode\"</li> <li>Please do not leave the PROPixx projector in \"awake\" mode when not in use e.g. overnight!<ul> <li>NOTE: <code>ppx a / ppx s</code> didn't work on one occasion. PROPixx needed a full power off/on to reset.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Check that the stimuli and responses are as expected.</li> <li>Check arrival of triggers in MEG recording.<ul> <li>NOTE: New Stim PC PP card has a different Base Memory Address than the OLD Stim PC.<ul> <li>Change any, e.g. MATLAB, code on the NEW Stim PC, referencing the PP Base Memory Address to...<ul> <li>CFF8 ... e.g. in <code>initialiseParallelPort.m</code></li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p> Start subject preparation, items to have ready/available.</p> <ul> <li>Label electrodes for EOG (2), ECG (2) and any other (i.e. EMG) electrodes. Have some additional ready in case you have to redo them.</li> <li>NOTE: Reusable and disposable electrodes are available.</li> <li>Electrode gel (in 20ml syringes - use the caulking gun to fill them.) We also have 12ml curved tip syringes now available.</li> <li>Cut tape ready for attaching electrodes to skin (Tegaderm tape)</li> <li>Tape for securing electrodes (Micropore tape) and small double-sided discs.</li> <li>If using an EEG cap, check it over for damaged electrodes. Makes sure it's clean and dry.</li> <li>Set up the Digitizer chair, away from any metal. Attach the receiver cube to the back of the chair. The cable has to point downwards.</li> <li>The locations of our various consumable items can be found here</li> </ul> </li> <li> <p> Check the MSR for any unwanted items that could cause artefacts and remove them.</p> </li> <li> <p> Check that the participant monitoring camera and microphone are working correctly.</p> </li> </ul>"},{"location":"meg/acquisition/meg-acquisition-checklist/#prepare-meg-system","title":"Prepare MEG system","text":"<ul> <li> Prepare system.</li> <li> Check quality of channels.<ul> <li>Click \"Noisy Channels\" for more information</li> </ul> </li> <li> Select or create project.<ul> <li>Check or select acquisition parameters.</li> <li>Check or create online averaging parameters'</li> </ul> </li> <li> Create subject.</li> </ul>"},{"location":"meg/acquisition/meg-acquisition-checklist/#prepare-subject","title":"Prepare subject","text":"<ul> <li> Offer bathroom break.</li> <li> Explain preparation procedure.</li> <li> Explain experiment.</li> <li> Let subject read and sign informed consent.</li> <li> Have subject remove metal objects, and do a comprehensive check with both metal detectors. Subject to change into scrubs if necessary (show subject to Changing Room and show scrubs sizes that are available).<ul> <li>We are trying to reduce our laundry bill so only use scrubs if necessary e.g. metal in clothing.</li> </ul> </li> <li> cClean hands with Alcohol Gel Sanitizer.</li> </ul>"},{"location":"meg/acquisition/meg-acquisition-checklist/#attach-electrodes","title":"Attach electrodes","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#check-impedance-of-electrodes","title":"Check impedance of electrodes","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#attach-hpi-coils","title":"Attach HPI coils","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#digitize-head-coordinate-system","title":"Digitize head-coordinate system","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#digitize-hpi-coils","title":"Digitize HPI coils","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#digitize-headshape","title":"Digitize headshape","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#prepare-the-subject-in-the-msr","title":"Prepare the subject in the MSR","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#if-using-the-eyelink-1000-plus","title":"If using the EyeLink 1000 Plus","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#start-recording","title":"Start recording","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#finishing-recording-session","title":"Finishing recording session","text":""},{"location":"meg/acquisition/meg-acquisition-checklist/#tidying-up","title":"Tidying Up","text":""},{"location":"meg/analysis/mne/","title":"MNE Python","text":"<p>MNE-Python is a Python package for analysing electrophysiology (MEG, EEG, sEEG, ECoG, NIRS, etc) data.</p>"},{"location":"meg/analysis/mne/#mne-python-versions","title":"MNE-Python Versions","text":"<p>BEAR Apps has several versions of MNE-Python as modules.</p>"},{"location":"meg/analysis/mne/#bear-modules","title":"BEAR Modules","text":"<p>The following <code>bash</code> loads <code>mne</code> version 1.3.1 and its dependencies - an equivalent is availiable for JupyterLab.</p> <pre><code>module load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\n</code></pre> <p>Note</p> <p>Note that MNE-Python depends on a number of other python applications that will be loaded automatically. The above code will also load <code>numpy</code>, <code>scipy</code>, <code>numba</code>, <code>matplotlib</code>, <code>sklearn</code> and many other packages that are needed by MNE into your environment.</p> <pre><code>import mne\nimport numpy as np\n</code></pre>"},{"location":"meg/analysis/mne/#mne-in-a-virtual-environment","title":"MNE in a virtual environment","text":"<p>If you want to use a specific version of MNE-Python that isn't supported by BEAR, you can install it into a virtual environment. This <code>bash</code> script provides an example. We load Python 3.9.5, create an environment and install MNE into the environment using <code>pip</code>.</p> <pre><code>#!/bin/bash\n\nmodule purge;\nmodule load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\nmodule load IPython/7.25.0-GCCcore-10.3.0\n\nexport VENV_DIR=\"${HOME}/virtual-environments\"\nexport VENV_PATH=\"${VENV_DIR}/mne-example-${BB_CPU}\"\n\n# Create master dir if necessary\nmkdir -p ${VENV_DIR}\necho ${VENV_PATH}\n\n# Check if virtual environment exists and create it if not\nif [[ ! -d ${VENV_PATH} ]]; then\n    python3 -m venv --system-site-packages ${VENV_PATH}\nfi\n\n# Activate virtual environment\nsource ${VENV_PATH}/bin/activate\n\n# Any additional installations\npip install mne==1.1.0\n</code></pre> <p>As with other examples, this can be copied directly into the terminal or saved as an shell script that can be executed in a terminal.</p>"},{"location":"meg/analysis/mne/#evoked-response-example","title":"Evoked response example","text":"<p>The following code is adapted from the MNE-Python overvew of MEG/EEG analysis tutorial. It will download a small example file and run a quick analysis.</p> <p>You can run this code directly in a Python session on BEAR within an activated MNE environment. The file will be saved into your RDS home directory (eg <code>/rds/homes/q/quinna</code>) unless specified otherwise - please change the <code>sample_data_folder</code> variable if you'd like to save this file elsewhere.</p> <pre><code>import numpy as np\nimport mne\n\n# This will download example data into your RDS home directory by default -\n# change the next line if you want to save the file elsewhere!\nsample_data_folder = '/rds/homes/q/quinna/mne-data/'\nsample_data_raw_file = (sample_data_folder / 'MEG' / 'sample' /\n                        'sample_audvis_filt-0-40_raw.fif')\nraw = mne.io.read_raw_fif(sample_data_raw_file)\n\nevents = mne.find_events(raw, stim_channel='STI 014')\n\nevent_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,\n              'visual/right': 4, 'smiley': 5, 'buttonpress': 32}\n\nreject_criteria = dict(mag=4000e-15,     # 4000 fT\n                       grad=4000e-13,    # 4000 fT/cm\n                       eeg=150e-6,       # 150 \u00b5V\n                       eog=250e-6)       # 250 \u00b5V\n\nepochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5,\n                    reject=reject_criteria, preload=True)\n\nconds_we_care_about = ['auditory/left', 'auditory/right',\n                       'visual/left', 'visual/right']\nepochs.equalize_event_counts(conds_we_care_about)  # this operates in-place\naud_epochs = epochs['auditory']\nvis_epochs = epochs['visual']\n\nvis_evoked = vis_epochs.average()\n\nfig = vis_evoked.plot_joint()\nfig[0].savefig('my-mne-evoked-example-grad.png')\nfig[1].savefig('my-mne-evoked-example-mag.png')\nfig[2].savefig('my-mne-evoked-example-eeg.png')\n</code></pre> <p>We can save this as <code>mne_python_example.py</code> as our core analysis script. This can be executed from a terminal session in which the appropriate python environment has been loaded. For example, we could open a 'BlueBEAR GUI' session, open a new terminal, change directory to the location of our script and run the following code:</p> <pre><code>module load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\n\npython mne_python_example.py\n</code></pre> <p>At the end you should have some new figures created next to your script.</p> <p> </p>"},{"location":"meg/analysis/mne/#mne-on-the-cluster","title":"MNE on the cluster","text":"<p>Warning</p> <p>There is currently a bug with this example to do with the automatic file downloading when running on the cluster. Running a similar example on your own data from RDS should work fine.</p> <p>Alternatively, we can run our evoked responses analysis on the cluster. For this we'll need a job submission script. Let's use the following shell code to load the MNE module from BEAR and run our code.</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n\nmodule purge; module load bluebear\n\nmodule load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\n\npython mne_python_example.py\n</code></pre> <p>Note</p> <p>We could equally create or load a virtual environment with a customised Python set-up in our job script. Here we use the built in BEAR module as it is all we need for the case in hand.</p> <p>We can save the job script as <code>run_mne_python_example.sh</code> and submit it to the cluster using <code>sbatch</code>:</p> <pre><code>sbatch run_mne_python_example.sh\n</code></pre> <p>You can see the progress of the job using the 'Active Jobs' page on BEAR portal or by reading the log files.</p>"},{"location":"meg/hardware/meg/","title":"Basic Workings of MEG","text":"<p>Magnetoencephalography (MEG) measures the magnetic fields generated by electric currents in the brain. When neurons are activated synchronously they generate electric currents and thus magnetic fields, which are then recorded by MEG outside the head. Unlike electrical currents (like in EEG), magnetic fields pass through the head without any distortion. Since this is a passive measurement, MEG is non-invasive.</p> <p>Recording the minute magnetic fields of the brain creates two challenges. One is to create a superconducting detector and the other is to attenuate external magnetic noise.</p> <p>The technology that helps record the minute magnetic fields is a superconducting quantum interference detector (SQUID) which is like a highly sensitive magnetic field meter. To maintain superconductors one needs to provide an extremely cold environment, which is achieved by using liquid helium around the sensors. At the CHBH we use an integrated closed-loop helium reliquefaction system which collects the helium gas that boils off during testing and liquefies this back when the MEG is not in use. The system, therefore, needs regular downtime in the evenings/weekends.</p> <p>Our MEG laboratory houses an Elekta Neuromag TRIUX system, reinstalled in January 2019.</p>"},{"location":"meg/hardware/meg/#elekta-neuromag-triux","title":"Elekta Neuromag TRIUX","text":"<p>The TRIUX system has 306 sensors distributed over the head:</p> <ul> <li>204 planar gradiometers.</li> <li>102 magnetometers.</li> </ul> <p>The MEG system is situated in a two-layer Magnetically Shielded Room (MSR), installed by VAC. The system allows for concurrent EEG recordings from 64 electrodes and continuous monitoring of the head position. A closed-loop He recycler eliminates refills.</p>"},{"location":"meg/hardware/safety/","title":"Shielded Room","text":"<p>The usual amplitude of magnetic fields created by the brain are extremely small, they do not exceed a few hundred femto tesla (10\u221215 T). Compared with this the Earth's magnetic field is between 10\u22124 and 10\u22125 T and an MRI is usually 1.5-3 T.</p> <p>Ross Devlin, Senior Service Specialist, MEGIN, says ... The background earth\u2019s magnetic field is about 0.5 Gauss or 50micro T = 50x10-3 T. This is 11 orders of magnitude higher than expected brain signals (10-14) If the sensors were able to function in that level of field without being completely saturated, which is the most likely scenario, then the background noise would be so high as to make the data unusable.</p> <p>To attenuate the external magnetic noise the MEG is housed inside a magnetically shielded room (MSR). An MSR is an enclosure with a shell comprising layers of high permeability metals that are also good electrical conductors. This attenuates (absorbs) the spurious magnetic and electrical fields emanating from outside sources. All objects within the room are non-metallic/non-magnetic. It is imperative that neither the MEG operator, nor the participants bring any metal or magnetic objects into the MSR. This is important for two reasons:</p> <p>Firstly, the sensors (SQUIDs) are ultrasensitive in the sense that they can pick up changes in magnetic field in the range of femtoteslas. Bringing magnetic objects close to the MSR can cause flux traps in the SQUIDs. This may lead to a long and expensive service break and delay the measurements.</p> <p>Secondly, bringing metal/magnetic objects into the MSR will distort your measurements.</p> <p>Therefore, before entering the MSR, please remove any potentially magnetic objects that you may be carrying. These include belts, keys, watches, coins, hairpins, eyeglasses and pieces of clothing with metallic parts. Also bringing cameras, flashlights, mobile phones or any other electrical equipment inside the MSR is strictly forbidden. If you want to use any untested equipment or objects (e.g. chairs, cushions, etc.) this has to be tested first, in the presence of an experienced MEG operator only. In the test, somebody waves the study object inside the closed MSR and somebody else watches the raw data display. Start from the door and then proceed towards the MEG little by little. If your colleague sees artifacts on the raw data display, then the object is magnetic and you should avoid bringing it inside the MSR.</p> <p>Please check whether your participant has removed all of the following items before entering the MSR:</p> <ul> <li>Phones</li> <li>Keys</li> <li>Bra\u2019s containing metal (e.g. underwired bra\u2019s) @</li> <li>Eye make-up containing metals (e.g. eye shadow/ mascara)</li> <li>Hair clips/pins</li> <li>Earrings and piercings</li> <li>Rings</li> <li>Bracelets/ necklaces containing metal</li> <li>Clothing with metal zips &amp; metal buttons</li> <li>@ Hospital scrubs available if dresses/jeans need to be changed out of.</li> <li>Any other metal objects in pockets, on clothing, or in hair</li> </ul> <p>Ask your participant if they have a dental retainer/brace, amalgam(mercury/metal alloy) fillings, or are wearing coloured contact lenses, or \"Quick Attach\" eyelashes.</p>"},{"location":"mri/","title":"MRI at CHBH","text":"<p>Guides and documentation for MRI analysis and data collection at the CHBH.</p> <ul> <li> <p>Technical Information</p> <p>Essential hardware and infrastructure details</p> <ul> <li>The MRI</li> <li>Recording data</li> <li>Troubleshooting</li> </ul> </li> <li> <p>Data analysis</p> <p>Analysis and data quality procedures</p> <ul> <li>fMRIPrep</li> <li>Freesurfer</li> <li>FSL</li> <li>SPM</li> </ul> </li> <li> <p>Mock MRI</p> <p>Equipment and Procedures</p> <ul> <li>Mock MRI</li> </ul> </li> </ul> <p>CHBH Sharepoint</p> <p>These pages contains public facing information about MRI at CHBH. For details on projects codes, approvals, ethics, finances, bookings and administration please see the CHBH Sharepoint pages (UoB SSO login required)</p>"},{"location":"mri/analysis/fmriprep/","title":"fMRIPrep","text":"<p>fMRIPrep is an open-source tool for preprocessing functional MRI (fMRI) data. It automates the early stages of MRI data preprocessing, including motion correction, susceptibility distortion correction, and slice-timing correction, providing a standardised and reproducible analysis pipeline. Unlike other preprocessing pipelines, fMRIPrep utilizes functions from many popular neuroimaging tools, including FSL, FreeSurfer, and AFNI, ensuring that each step in the pipeline uses the most reliable and validated methods available, resulting in higher-quality outputs.</p>"},{"location":"mri/analysis/fmriprep/#downloading-fmriprep","title":"Downloading fMRIPrep","text":"<p>The simplest way of running fMRIPrep is using a container. Instructions for downloading the fMRIPrep container are detailed here.</p>"},{"location":"mri/analysis/fmriprep/#running-fmriprep","title":"Running fMRIPrep","text":"<p>In order to run fMRIPrep, your data must first be organised according the Brain Imaging Data Structure BIDS guidelines. Once organised into BIDS, the default fMRIPrep container can be run using the following script:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem 18G\n\nbids_directory=camcan_bids/\noutput_directory=camcan_fmriprep/\n\napptainer run fmriprep_24_1_1.sif ${bids_directory} ${output_directory} participant -w work/ --participant-label 01 --fs-license-file ~/license.txt \n</code></pre> <p>This script can then be submitted using the following command:</p> <pre><code>sbatch fmriprep.sh\n</code></pre> <p>Descriptions of the variables and arguments:</p> Variables / Arguments Description <code>bids_dir</code> First positional argument. Path to the BIDS-formatted dataset. <code>analysis_level</code> Second positional argument. Should be set to \"participant\" (required). <code>output_dir</code> Third positional argument. Sets the output directory. <code>-w</code> Specifies the working directory to store intermediate files during preprocessing. <code>--participant-label</code> Specifies the BIDS subject ID, enabling fMRIPrep to run on a single subject or a subset of subjects. <code>--fs-license-file</code> Path to the FreeSurfer license file (see FreeSurfer). By default, fMRIPrep uses FreeSurfer for anatomical co-registration. <code>--nprocs</code> Number of CPU cores to use for processing. <code>--mem</code> Amount of memory available in GB. <p>These arguments represent only a selection of the available options.</p>"},{"location":"mri/analysis/fmriprep/#running-fmriprep-across-all-subjects","title":"Running fMRIPrep Across all Subjects","text":"<p>It is also possible to run all subjects within a BIDS directory by using the following modified script:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem 18G\n#SBATCH --array=0-20 # Number of subjects in the BIDS directory.\n\nSUBJECTS=($(find camcan_bids -maxdepth 1 -type d -name 'sub-*' | sort | xargs -n 1 basename))\nSUBJECT_ID=${SUBJECTS[$SLURM_ARRAY_TASK_ID]}\nbids_directory=camcan_bids/\noutput_directory=camcan_fmriprep/\n\napptainer run fmriprep_24_1_1.sif ${bids_directory} ${output_directory} participant -w work/ --participant-label ${SUBJECT_ID} --fs-license-file ~/license.txt \n</code></pre> <p>Note</p> <p>The number at the end fo the <code>#SBATCH --array=0-20</code> should be replaced with the number of subjects within the BIDS directory.</p>"},{"location":"mri/analysis/freesurfer/","title":"FreeSurfer","text":"<p>FreeSurfer is an open-source package for the analysis and visualization of structural, functional, and diffusion neuroimaging data from cross-sectional and longitudinal studies.</p>"},{"location":"mri/analysis/freesurfer/#freesurfer-license","title":"FreeSurfer License","text":"<p>FreeSurfer requires a license registration key in order to be used. This can be obtained from here. Once downloaded, the file should be uploaded to your home directory on Bear. This can be done using \"Files\" tab on the BlueBEAR portal, or using file transfer software, such as WinSCP, or FileZilla. </p>"},{"location":"mri/analysis/freesurfer/#running-recon-all","title":"Running recon-all","text":"<p>The <code>recon-all</code> command performs all, or any part of, the FreeSurfer cortical reconstruction process. The outputs of <code>recon-all</code> can be used to define the surfaces required for the boundary estimate model (BEM) required when performing source reconstruction on M/EEG data. The function can be run via BlueBEAR using the example script (recon_all.sh) below:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem-per-cpu 2\n\nmodule purge\nmodule load bear-apps/2019a/live\nmodule load FreeSurfer/6.0.1-centos6_x86_64\n\nexport FS_LICENSE=${HOME}/license.txt\nexport SUBJECTS_DIR=/rds/projects/b/bagshaap-eeg-fmri-hmm/fs_outputs\n\nrecon-all -s sub-01 -i /rds/projects/b/bagshaap-eeg-fmri-hmm/T1_vol_v1_5.nii.gz \\\n-all \\ \n-log logfile \\ \n-all \\\n-parallel -openmp 4\n</code></pre> <p>This script can then be submitted using the following command:</p> <pre><code>sbatch recon_all.sh\n</code></pre> <p>Descriptions of the variables and arguments:</p> Variables / Arguments Description <code>FS_LICENSE</code> Sets the path to the FreeSurfer license file. <code>SUBJECTS_DIR</code> Sets the output directory for the analysis. <code>-s</code> Sets the name of the output folder. <code>-i</code> Specifies the full path to the T1-weighted MRI image. <code>-all</code> Instructs FreeSurfer to run all processing steps. <code>-log</code> Creates a log file named \"logfile\" upon completion of the processing. <code>-parallel</code> Enables parallel processing in FreeSurfer. <code>-openmp</code> Defines the number of CPU cores available for parallel processing. <p>For the above script to work for you, several of the variables and arguments need to be changed to match your filenames and directories. Specifically, the <code>SUBJECT_DIR</code> variable needs to be changed to the path where you want the outputs to be saved, the <code>-s</code> argument needs to be changed to the desired name of the output folder, and the <code>-i</code> argument needs to be changed to a path to your T1 file. </p>"},{"location":"mri/analysis/freesurfer/#running-recon-all-on-multiple-subjects","title":"Running recon-all on Multiple Subjects","text":"<p>Alternatively, if you need to run recon-all for multiple subjects at once, for example, on an entire BIDS dataset, it is possible to submit all jobs using the below script (recon_all_bids.sh):</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem-per-cpu 2\n#SBATCH --array=1-20 # 20 represents the total number of subjects\n\nmodule purge\nmodule load bear-apps/2019a/live\nmodule load FreeSurfer/6.0.1-centos6_x86_64\n\nexport FS_LICENSE=${HOME}/license.txt\nexport SUBJECTS_DIR=/rds/projects/b/bagshaap-eeg-fmri-hmm/fs_outputs\n\nsubject_id_number=$(printf \"%02d\" ${SLURM_ARRAY_TASK_ID})\n\nrecon-all -s sub-${subject_id_number} \\\n-i /rds/projects/b/bagshaap-eeg-fmri-hmm/bids_dataset/sub-${}/anat/sub-${subject_id_number}_T1w.nii.gz  \\\n-all \\ \n-log logfile \\ \n-all \\\n-parallel -openmp 4\n</code></pre>"},{"location":"mri/analysis/freesurfer/#running-freesurfer-in-a-container","title":"Running FreeSurfer in a Container","text":"<p>Running FreeSurfer within a container allows for greater control over the software versions and improves the reproducibility of the analysis. FreeSurfer containers are available on Dockerhub, or can be created using NeuroDocker (see the Containers page for details on downloading containers). </p> <p>In the example below we assume a container named <code>freesurfer.sif</code> has been downloaded:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem-per-cpu 2\n\nFS_LICENSE=${HOME}/license.txt\nSUBJECTS_DIR=/rds/projects/b/bagshaap-eeg-fmri-hmm/Projects/Visual_Response_Variability/fs_outputs\n\napptainer exec --env FS_LICENSE=${FS_LICENSE} --env SUBJECTS_DIR=${SUBJECTS_DIR} \\\nfreesurfer.sif \\\nrecon-all -s sub-01 -i /rds/projects/b/bagshaap-eeg-fmri-hmm/T1_vol_v1_5.nii.gz \\\n-log logfile \\ \n-all \\\n-parallel -openmp 4\n</code></pre>"},{"location":"mri/analysis/fsl/","title":"FSL","text":"<p>FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.</p>"},{"location":"mri/analysis/fsl/#fsl-modules","title":"FSL Modules","text":"<p>A range of installed FSL versions are available as modules on Bear Apps.</p>"},{"location":"mri/analysis/fsl/#bear-portal-gui","title":"BEAR Portal GUI","text":"<p>The following code snippet can be executed in a terminal from within the Bear Portal GUI. It will load a pre-installed FSL version into the terminal where is can be used as normal.</p> <pre><code>module load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n</code></pre> <p>We can then use FSL command line functions as normal:</p> <pre><code>fsl_anat --help\n</code></pre> <p>or open the FSL GUI:</p> <pre><code>fsl\n</code></pre>"},{"location":"mri/analysis/fsl/#fsleyes-on-bear-portal-gui","title":"FSLEyes on BEAR Portal GUI","text":"<p>FSLEyes is the MRI volume visualisation tool provided and maintained by the FSL team. This runs well in BEAR GUI and can be added to the local environment by adding the following module. (See the FSLEyes page on BEAR Apps for all available versions). </p> <pre><code>module load FSLeyes/1.3.3-foss-2021a\n</code></pre> <p>Once the module has loaded you can run FSLEyes from the terminal as normal.</p> <pre><code>fsleyes\n</code></pre> <p>Info</p> <p>Note that it is currently not possible to have both FSL and FSLEyes in the environment in the same terminal. Until this is fixed, please use two separate terminal sessions, one for FSL and one for FSLEyes.</p>"},{"location":"mri/analysis/fsl/#fsl-on-the-cluster","title":"FSL on the cluster","text":"<p>We can also run FSL jobs on the cluster using job scripts. The following can be saved as <code>run_fsl_bet.sh</code> and submitted to the cluster using <code>sbatch</code>. This will run a brain extraction on a single datafile.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account quinna-camcan\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 5 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n\nmodule purge\nmodule load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n\nset -e\n\nbet subject1.nii.gz subject1_brain.nii.gz\n</code></pre> <p>If we have many datafiles to run BET on, we can extend our script into an array job. This is a <code>slurm</code> script that actually creates many jobs that can be run in parallel. Here we add the <code>#SBATCH --array=1-48</code> line to our script to tell it that we want to parallelise our script across the range 1 to 48. This creates 48 separate jobs each with a value between 1 and 48 stored in the variable <code>${SLURM_ARRAY_TASK_ID}</code>. Our <code>BET</code> call changes the subject number with this variable for each job.</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-camcan\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 5 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n#SBATCH --array=1-48\n\nmodule purge\nmodule load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n\nset -e\n\nbet subject${SLURM_ARRAY_TASK_ID}.nii.gz subject${SLURM_ARRAY_TASK_ID}_brain.nii.gz\n</code></pre> <p>Submitting this script to the cluster will run BET 48 times on each input from <code>subject1.nii.gz</code> to <code>subject48.nii.gz</code>.</p>"},{"location":"mri/analysis/fsl/#fsl-in-a-container","title":"FSL in a container","text":"<p>Sometime we may want more control over software versions that are supported by pre-compiled BEAR App. We can install FSL within a controlled container using the following job script. This creates a container file <code>FSL.sif</code> from the NeuroDesk container specification.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 10 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n#SBATCH --constraint icelake\n\nset -e\n\napptainer pull --name FSL.sif docker://vnmd/fsl_6.0.4\n</code></pre> <p>We can submit this job to the cluster using <code>sbatch</code> as normal. Once the <code>FSL.sif</code> has been created we can run future cluster jobs through it.</p> <p>For example, this job script runs <code>fsl_anat</code> on a single dataset using our FSL container.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account quinna-camcan\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 5 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n\nmodule purge\nmodule load bluebear\n\nset -e\n\napptainer exec FSL.sif  fsl_anat -i subject1.nii.gz -o subject1\n</code></pre> <p>This can be combined with array jobs from the example above to run many container-based analyses together in parallel.</p>"},{"location":"mri/analysis/spm/","title":"SPM12","text":""},{"location":"mri/analysis/spm/#running-first-level-analyses-with-parfor","title":"Running first-level analyses with parfor","text":"<p>Info</p> <p>Example contributed by Arkady Konovalov</p> <p>Simple parallelisation of a for-loop can be performed using parfor. This functionality is provided by MATLAB and enables faster processing of <code>for</code> loops simply by changing the syntax at the start to say <code>parfor</code> rather than <code>for</code>.</p> <p>Here is an example function which makes use of <code>parfor</code> whilst computing GLMs using SPM.</p> <pre><code>function glm_level1(model)\n% This function takes a model structure as input and performs first-level\n% estimations in a General Linear Model (GLM) analysis for a set of subjects.\n\nsubjects = model.Subj;\n\n% FIRST LEVEL (individual) estimations\n% Get the number of subjects to be processed.\nN = size(subjects,2);\n\n% Iterate over each subject in \"subjects\" using parallel processing\nparfor i = 1:N\n\n        % Get the current subject ID from \"subjects\"\n        id = subjects(i);\n\n        % Get the corresponding BIDS (Brain Imaging Data Structure) ID and\n        % session information for the current subject.\n        BIDS_id = model.ids{id};\n        BIDS_sess = model.sess{id};\n\n        % Construct the path to the GLM folder for the current subject.\n        path = [model.glmfolder BIDS_id];\n\n        % Construct the path to the SPM.mat file for the current subject.\n        modelfile = [path '/SPM.mat'];\n\n        % Delete the existing SPM.mat file for the current subject (clean\n        % up previously done models)\n        delete(modelfile);\n\n        % Create a job structure for the current subject.\n        job = analysis_job_func(BIDS_id, BIDS_sess, model);\n\n        % Create an empty cell array to be used as inputs for the \"spm_jobman\" function.\n        inputs = cell(0,1);\n\n        % Set the SPM defaults to 'FMRI'.\n        spm('defaults', 'FMRI');\n\n        % Run the current job using the \"spm_jobman\" function.\n        spm_jobman('run', job, inputs{:});\n\nend\n\nend\n</code></pre> <p>Note</p> <p>Make sure you specify the appropriate number of cores when starting the MATLAB GUI App, you may not notice a substantial speed-up if you run MATLAB using the default of 4 cores. Do try to avoid asking for substantially more than you might need however - BlueBEAR is a shared resource!</p>"},{"location":"mri/hardware/scanner/","title":"MRI Specifications and Equipment","text":"<p>Magnetic Resonance Imaging (MRI) is a medical imaging technique that uses strong magnetic fields and radio waves to generate detailed images of organs and tissues inside the body. The CHBH houses a SIEMENS MAGNETOM PRISMA 3T installed in January 2019. </p> <p>The scanner can be used to assess brain activity and function through several techniques:</p> <ul> <li> <p>Structural MRI is used to produce high-resolution anatomical images</p> </li> <li> <p>Functional MRI (fMRI) measures brain activity by detecting changes in blood oxygen levels</p> </li> <li> <p>Diffusion Tensor Imaging (DTI) maps brain connectivity through white matter tracts</p> </li> <li> <p>Magnetic Resonance Spectroscopy (MRS) measures levels of molecules including metabolites and neurotransmitters</p> </li> </ul> <p>Info</p> <p>The CHBH houses 20-channel, 32-channel and 64-channel head coils.</p> <p>The CHBH also has equipment for combining MR with simultaneous behavioural and physiological assessment. </p> <ul> <li> <p>Eyetracking - SR Eyelink 1000 tracker</p> </li> <li> <p>Respiratory belt, electromyography and pulse monitoring - SIEMENS Healthineers</p> </li> <li> <p>Grip force transducer - BIOPAC MP160 with AcqKnowledge software</p> </li> <li> <p>Button box - fMRI Button Pad (2-Hand) System (NATA Technologies)</p> </li> </ul>"},{"location":"mri/hardware/stimulus_equipment/","title":"Stimulus Equipment","text":""},{"location":"mri/hardware/stimulus_equipment/#stimulus-pcs","title":"Stimulus PCs","text":"<p>The CHBH has a rack of Stimulus PCs in the Console Room. The individual PCs are addressed by a single keyboard mouse &amp; local screen via the KVM Switch (Keyboard/Video/Mouse Switch) mounted at the top of the rack.</p> <p>Choose which Stimulus PC you want to run your experiment on and select that via the KVM Switch. KVM1 for Windows 10, KVM 2 for NeuroDebian. you can freely switch between the two, though MATLAB may need to be relaunched if it was active during switching.</p> <p>What is seen on the local screen in the console room, is seen on the projector screen (when the wake command has been issued to the projector). Both local terminal screen and VPixx Projector are 1920x1080@120Hz for Ubuntu, and 1920x1080@100Hz for Windows 10 (not the 100Hz for Windows, 120Hz is not stable and will cause flickering). Note that only the central element of the Console Screen will be visible on the in bore projection screen. The Projector overfills the rear projection screen and the useful resolution is closer to ~4:3 ~1440x1080 visible. Always verify your paradigm's visual elements are not only visible on the projector screen and stimulus machine monitor, but remain so via the participant's superior mirror.</p> <p>The KVM is not configured to route Audio, therefore to choose a specific Stimulus PC, as audio source, employ the plugs on the Audio Patch Panel.</p> <p>Available Stimulus PCs are;</p> <ul> <li> <p>KVM 1 - Windows 10 - PsychoPy, MATLAB 2018B with the PTB, Presentation, EPrime and NIDAQ Hardware [[6]]</p> </li> <li> <p>KVM 2 - Ubuntu 18.04.1 LTS with Neurodebian - PsychoPy, MATLAB 2018B with the PTB [[7]]</p> </li> </ul> <p>These have near identical hardware, INTEL i7, AMD WX7100. The Windows 10 machine has an additional National Instruments DAQ and BNC breakout attached. If one machine fails, the other can boot to Windows 10 / Ubuntu with intervention of the Technician. Identical Common Experimental Platform PCs are also installed in other modalities, several cubicles and the Mock Scanner room. You are invited to test your paradigms there.</p> <p>There are also legacy Stimulus PCs at the foot of the rack.</p> <ul> <li> <p>KVM 3 - Windows 7 - this is a former BUIC STIM PC and should function as before, except a difference in screen resolution* {Currently disabled and soon to be retired}</p> </li> <li> <p>KVM 4 - Windows XP - QUALISYS, ROBOT, other legacy.</p> </li> </ul> <p>When any CHBH Stimulus PC is selected via the KVM it will receive reports from the Keyboard, Mouse at the local screen, and from the NATA Response Interface and the LABJACK U3. Other devices may be plugged directly into the fascia of the desired Stimulus PC.</p>"},{"location":"mri/hardware/stimulus_equipment/#vpixx-projector","title":"vPixx Projector","text":"<p>The commands needed to enable the projector are as follows;</p> <ul> <li> <p>launch the VPUtil command line interface (click the desktop icon in Windows / under Linux; click to open the desktop VPUtil folder, and in the folder pane that opens rightclick on the white space and Launch A Terminal From That Location, then type ./vputil and press enter).</p> </li> <li> <p>In that VPUtil interface type ppx a and return to awaken the projector.</p> </li> <li> <p>In that VPUtil interface type ppx s and return to deactivate.</p> </li> <li> <p>Do not leave the projector awakened.</p> </li> <li> <p>If you get the error 'VPixx Device FPGA device appears unprogrammed' this means the USB connection to the Projector has failed. Switch the KVM to 2 or and then back to 1, or vice versa, for 10 seconds to restore connection. You can switch between Windows 10 and the NeuroDebian CEP using the KVM, whilst the projector is live, and the projector will display the selected desktop.</p> </li> </ul> <p>These should be all you need for basic use of the VPixx. Instructions with detailed trouble shooting steps are found here. UPDATED SEPT 2021 - if the projector / console screen is flickering for more than a minute after booting you may need to reboot.</p> <p>The projector should be set at 16:9 1920x1080@120Hz. The Projector overfills the rear projection screen and the useful resolution is closer to 4:3 1440x1080 visible. Always verify your paradigm's visual elements are; not only visible on the projector screen and stimulus machine monitor, but also remain so via the participant superior mirror.</p>"},{"location":"mri/hardware/stimulus_equipment/#recovering-responsestriggers-via-nata-interface","title":"Recovering responses/triggers via NATA Interface","text":""},{"location":"mri/hardware/stimulus_equipment/#recovering-triggers","title":"Recovering Triggers","text":"<p>Scanner volume timing is delivered to the CEP Stimulus machines by the detection of the letter 't' (as if from a USB keyboard, spoofed by the NAtA interface box). This will echo into any command windows you use for running stimulus. Consider using the 'commandwindow' command at the beginning of MATLAB scripts to direct key presses away from the script editor! Alternatively switch off the NAtA interface box when troubleshooting, or open a notepad window to redirect these.</p> <p>When a Stim PC is selected via the KVM [[9]] and the 2x5 NATA Response Interface is on, [[10]], then scanner volume triggers will echo as 't' keypress (at ~7ms after the volume begins). Consider using the suggested trigger code snippet on github, linked below, to recover this timing. If you are recovering other keypresses (e.g. 0-9) make sure you ignore ensuing 't's. Beware also, that if CAPSLOCK is on any keyboard attached to the current Stim PC, the NAtA will report 'T'. So always check CAPS LOCK is off, or program defensively to respond to 't' or 'T'.</p> <p>KbCheck on Windows, and KbCheck(-3) on Linux should suffice to recover the 't' press. When you want to ignore successive 't' during recovery of Response Box keypresses use DisableKeysForKbCheck.</p> <p>Example code using the more complex KbQueue is provided here.</p> <p>The SIEMENS Prisma produces a 1us impulse which is delivered fibre-optically to the SIEMENS Interface Box behind the console PC. This has two settings, TOGGLE and IMPULSE, and requires USB power. Ensure IMPULSE is selected. The BNC output of this SIEMENS Interface Box is relayed to the NATA Response Interface, with potential breakout for other devices. </p> <p>The NAtA is connected directly to the KVM and reports a 't' via USB connection, which also reports 1,2,3,4,5 &amp; ,6,7,8,9.0 for NATA response peripheral. The NATA Interface Box also passes the signal via a DB25 breakout, and &lt;1ms delay. This is DB25 breakout is normally connected to the legacy Windows 7 BUIC Stimulus PC. This enables legacy BUIC experiments to be run without alteration on the legacy Stimulus PC (besides screen size considerations). The DB25 breakout may also be connected to the LABJACK U3, the National Instruments DAQ or the BlackBoxToolKit.</p>"},{"location":"mri/hardware/stimulus_equipment/#recovering-participant-responses-in-linux","title":"Recovering Participant Responses in Linux","text":"<p>The NAtA Response Pads are installed in the scanner chamber in two configurations. 2x2 which report 12 34 and 2x5 which report 12345 67890 as if they were cut down USB keyboards. (NB Responding as US keyboards in PTB, not UK configuration.)</p> <p>The 2x2 Interface and 2x5 are always on and connected. Both are connected via the KVM. if you switch between Windows and Linux whilst MATLAB is open, you may need to re-open MATLAB for to detect the NAtA.</p> <p>Under Neurodebian use the following device names when setting up your KbQueue in PTB MATLAB;</p> <pre><code>DeviceName = 'NAtA Technologies LxPAD PK080219 v6.11'  % 2x5 and trigger 't'\n\nDeviceName = 'NAtA Technologies LxNK Keypad' % 2x2 only\n\nDeviceName = 'Dell Dell QuietKey Keyboard'  % Experimenter Keyboard\n</code></pre> <p>Here is example code that demonstrates gathering Experimenter Keyboard Responses and Participant 2x5 / Scanner volume triggers from the NAtA 2x5.</p> <p>Always ensure the NAtA boxes respond before placing your participant in the scanner. They will illuminate the red LEDs on the front of the NATA Interface Box, so observe these. If they illuminate and there is no response from the stimulus PC then reset the PC as the USB hub has been sent to sleep and not awoke with the PC.</p>"},{"location":"mri/hardware/stimulus_equipment/#participant-audio-siemens-soundpixx","title":"Participant audio - Siemens / SoundPIXX","text":"<p>For Operator/Participant contact we rely on the inbuilt Siemens audio console. This broadcasts into the Room, and optionally via the mono pneumatic in-ear headphones that can be attached to the base of the participant bed.</p> <p>For experimental audio, routed from the CEP stimulus machines the Siemens Room Audio / In-Ear Headphones may be suitable. However if you require Stereo reproductions or and higher fidelity audio reproduction there is the SoundPIXX system. This requires the use of a second set of headphones, stored to the left hand side of the head of the scanner bed.</p> <p>As noted above, the KVM is not configured to route Audio, therefore to choose a specific Stimulus PC, as audio source, employ the plugs on the Audio Patch Panel.To switch between the two audio systems there is a patch panel (bank of ports for large audio cables) below the CEP Stimulus machines on the main rack. These allow audio to be routed from the CEP Stimulus Machines to the two available audio outputs. Sources are the Windows and the Neurodebian CEP Stim, outputs are the Siemens (Room + Participant Headphones), or the SoundPIXX (Participant Headphones only).</p> <p>When utilising the SoundPIXX Audio the Siemens Room audio may suffice for participant communication still, but there is a secondary black desktop Microphone that you can use to speak directly into the SoundPIXX headphones.</p> <p>Warning</p> <p>The SoundPIXX headphones have a much higher range of available volume than the Siemens in-Ear headphones and participants may be overwhelmed. Before any experiment involving their use check the settings are suitable, and that they have not been changed since your last scan. Additionally check the volume setting on our CEP Stimulus machine has not changed as the interaction of these two can produce surprisingly high volumes of noise which will be delivered directly into the SoundPIXX in ear headphones.</p>"},{"location":"mri/hardware/stimulus_equipment/#troubleshooting","title":"Troubleshooting","text":"<p>When the Siemens Room audio is all that is required the patch panel should be left in the standard position to avoid a ground loops developing - which delivers a rumbling bassy noise. Should you experience this noise switch the Audio patch from 23-24 or vice versa.</p>"},{"location":"mri/hardware/troubleshooting/","title":"Troubleshooting","text":""},{"location":"mri/hardware/troubleshooting/#vpixx-device-fpga-device-appears-unprogrammed","title":"VPixx Device FPGA device appears unprogrammed","text":"<p>Problem</p> <p>If you get the error 'VPixx Device FPGA device appears unprogrammed' this means the USB connection to the Projector has failed.</p> <p>Solution</p> <p>Switch the KVM to 2 or and then back to 1, or vice versa, for 10 seconds to restore the connection. You can switch between Windows 10 and the NeuroDebian CEP using the KVM, whilst the projector is live, and the projector will display the selected desktop.</p> <p>Instructions with detailed trouble shooting steps are found HERE. UPDATED SEPT 2021 - if the projector / console screen is flickering for more than a minute after booting you may need to reboot.</p>"},{"location":"mri/hardware/troubleshooting/#siemens-room-audio","title":"Siemens Room audio","text":"<p>Problem</p> <p>When the Siemens Room audio is all that is required the patch panel should be left in the standard position to avoid a ground loops developing - which delivers a rumbling bassy noise.</p> <p>Solution</p> <p>Should you experience this noise switch the Audio patch from 23-24 or vice versa.</p>"},{"location":"mri/hardware/troubleshooting/#amplifier-out-of-synch-barker-words-missing","title":"Amplifier Out of synch, Barker words missing!","text":"<p>Problem</p> <p>This error was encountered when using the Brain Products MR EEG caps and amplifiers in the PTL (G08). The user was having issues keeping the impedance low as well as the software cutting out with the error \"Amplifier out of sync, Barker words missing!\"</p> <p>Possible Solution</p> <p>The so-called 'Barker words' are part of a watchdog mechanism that assures the integrity of the communication between the amp and PC interface. The error message likely originates from the BrainAmp driver. The error message suggests that the optical path is impaired. Check the following ...</p> <p>Correct connection between fiber optic and amp/USB-adapter. Clean optical plugs/slots. If cleaning doesn't help, use another fiber optic cable for testing Clean ribbon cables, plugs, and sockets. Make sure the cut ends of the ribbon cable are cleaned of any dried gel. Update your BrainVision Recorder .</p> <p>NOTE: For a combined EEG-fMRI measurement only: If the \"Barker words missing!\" error coincides with the start of an fMRI sequence, it can also be a sign of an RF overload. An amp experiencing RF overload can produce arbitrary error messages in Recorder.</p>"},{"location":"mri/hardware/troubleshooting/#the-participant-logging-computer-plc-isnt-working","title":"The Participant Logging Computer (PLC) isn't working","text":"<p>If the Participant Logging Computer (PLC) is faulty, or if there is some other Service/Network issue, but our intranet is working, you should be able to use the logging software via a browser.</p> <p>Links are shown below. The second log screen should automatically show after participant details are entered into the first screen, but the second link is provided below if needed.</p> <p>NOTE: The links will only work on the MRI Stim PCs.</p> <p>https://www.chbh.bham.ac.uk/log/log-screen1.html</p> <p>https://www.chbh.bham.ac.uk/log/log-screen2.html</p>"},{"location":"mri/hardware/troubleshooting/#in-bore-projector-screen-out-of-focus","title":"In bore projector screen out of focus","text":"<p>Problem</p> <p>On occasion, the in bore projector screen is out of focus.</p> <p>Solution</p> <p>Most commonly the projector screen has been moved in the bore and not replaced in the correct locale. You can remedy this by ensuring the red stabilisers (pictured below) are aligned with the black circles inscribed in the scanner bore. General users and operators are free to move the screen to recover focus themselves. Deploy caution as the screen has a deformable membrane, and costs \u00a3ks to replace. Move it only via the solid base.</p> <p>Much more rarely the Projector in the Equipment room has been moved. Resetting the focus in that room should be the province of the CHBH technical staff.</p>"},{"location":"sleep/","title":"Sleep Research at CHBH","text":"<p>Guides and documentation for sleep data collection at the CHBH.</p> <ul> <li> <p>Technical Information</p> <p>Essential hardware and infrastructure details</p> </li> <li> <p>Data Acquisition</p> <p>Equipment and setup for data collection</p> </li> <li> <p>Stimulus Delivery</p> <p>Software and methods for stimulus presentation</p> </li> <li> <p>Quality Control</p> <p>Analysis and data quality procedures</p> </li> </ul> <p>CHBH Sharepoint</p> <p>These pages contains public facing information about sleep research at CHBH. For details on projects codes, approvals, ethics, finances, bookings and administration please see the CHBH Sharepoint pages (UoB SSO login required)</p>"},{"location":"software/","title":"Overview","text":"<p>This section collects tutorials and examples to run software on BlueBEAR. This is intended to extend the main BEAR Technical Documentation pages.</p> <p>Click on the links below to see the examples.</p> <ul> <li>Conda</li> <li>Containers</li> <li>MATLAB</li> <li>Python</li> <li>R</li> </ul>"},{"location":"software/R/","title":"R for statistical computing","text":"<p>The R project is a free software environment for statistical computing and graphics.</p>"},{"location":"software/R/#r-versions","title":"R Versions","text":"<p>BEAR Apps has several versions of <code>R</code> as loadable modules.</p>"},{"location":"software/R/#r-studio-gui-app","title":"R-Studio GUI App","text":"<p>RStudio is an integrated development environment (IDE) for <code>R</code>. It includes a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging, and workspace management.</p> <p>You can open an interactive RStudio session through the BEAR Portal. The pre-installed <code>R</code> versions can be loaded.</p>"},{"location":"software/R/#neuroimaging-specific-r-packages","title":"Neuroimaging specific R packages","text":"<p>Here is a list of <code>R</code> packages commonly used for neuroimaging analysis.</p>"},{"location":"software/R/#fslr","title":"FSLR","text":"<p>Wrapper functions that interface with 'FSL', a powerful and commonly-used 'neuroimaging' software, using system commands.</p>"},{"location":"software/R/#r-example-for-bear","title":"R Example for BEAR","text":"<pre><code>knitr::opts_chunk$set(echo = TRUE)\nlibrary(ggplot2)\n\n# Simulate some data\ntime     &lt;- 0:99\nset.seed(1)\nnoise    &lt;- rnorm(100)\ndisorder &lt;- time * 4 + 100 + noise * 20\ndis_df   &lt;- data.frame(time, disorder)\n\n# Create a plot\nggplot(dis_df, aes(x = time, y = disorder)) + geom_point() +\n  geom_smooth(method = \"lm\")\n\n# Fit model\nlm_fit &lt;- lm(disorder ~ time, dis_df)\nsummary(lm_fit)\n</code></pre>"},{"location":"software/conda/","title":"Using Conda to manage Python environments","text":"<p>Managing software dependencies can be one of the most annoying things in scientific computing, especially when dealing with complex systems with multiple packages and libraries.</p> <p>This is where <code>conda</code> comes in - a package management system that simplifies the task of installing, configuring, and managing software packages and their dependencies.</p> <p>BEAR systems offer a default Python environment, but it must be loaded beforehand, and it does not allow you to install additional packages that are not already provided. After creating your own Conda environment, you don't have to load them every time. Submitting cluster jobs becomes straightforward with simple scripts, as demonstrated in this example:</p> <pre><code>#!/bin/bash\nset -e\n#SBATCH settings\npython your_script.py\n</code></pre> <p>Note</p> <p>BEAR do not currently recommend using conda for installing Python packages. This is due to possible performance issues when installing packages with conda. BEAR recommend that you get in touch with them prior via an IT-ticket to performing intensive analyses using <code>conda</code> environments.</p> <p>If this seems interesting to you, or you need to use a custom environment, let's get started!</p>"},{"location":"software/conda/#what-is-conda","title":"What is <code>conda</code>","text":"<p>Conda is a platform-agnostic package management system that can be used to install and manage software packages and their dependencies. It is designed to work with multiple programming languages, including Python, R, and others.</p> <p>Advantages of using <code>conda</code>:</p> <ul> <li>Build your own Python env when you don't have <code>sudo</code> access</li> <li>Conda simplifies managing software packages and dependencies</li> <li>Conda allows for isolated environments for different projects or applications</li> <li>Conda facilitates switching between different package versions</li> <li>Conda provides access to a vast range of pre-built packages and libraries.</li> </ul>"},{"location":"software/conda/#how-to-install-conda","title":"How to install <code>conda</code>","text":"<p>First, go to your home directory and type:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n# start installation\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Follow the instructions on the screen to complete the installation (You can just leave everything as default, it should be fine most of the time). After the installation is complete, you need to restart your kernel to load it, type:</p> <pre><code>exec bash\n</code></pre> <p>And you should see your terminal changed, for example from <code>[&lt;usr&gt;@bb-pg-login04 ~]$</code> to <code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$</code>. This means you have a Python environment called <code>base</code> that is now active. Now you can start using conda to manage your own Python environments!</p>"},{"location":"software/conda/#use-base-environment","title":"Use <code>base</code> environment","text":"<p>If you accepted the default setting when installing, the <code>base</code> environment is the default environment, Every time you logged in to BEAR this environment will be loaded automatically.</p> <p>You can install any packages you need in the <code>base</code> environment with <code>pip</code> or <code>conda</code> command.</p> <p>For example, to install <code>matplotlib</code> and <code>scipy</code>:</p> <pre><code>conda install -c conda-forge matplotlib=3.5.2 scipy\n# or\npip install matplotlib==3.5.2 scipy\n</code></pre> <p>If you have the <code>requirements.txt</code> from the projects you are working with:</p> <pre><code>pip install -r requirements.txt\n# or \nconda install --file requirements.txt\n</code></pre> <p>After installing the packages, you can check the list of packages installed in the <code>base</code> environment with:</p> <pre><code>conda list\n# or \npip list\n</code></pre> <p>Congratulations! Now you can start using your own environment to do anything you like! Simply type:</p> <pre><code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$ python your_script.py\n# or \n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ python # to start a python shell\n# or \n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ ipython # to start a ipython shell\n</code></pre>"},{"location":"software/conda/#create-virtual-environments-with-conda","title":"Create virtual environments with <code>conda</code>","text":"<p>In most cases, the <code>base</code> environment should be enough. But if you are working on multiple projects, especially when you have deep learning projects or developing a toolbox that might be sensitive to specific environmental conditions, it is important to take measures to ensure consistency and prevent any problems that could arise from changes in the environment.</p> <p>Type the following command to create and enter a new environment:</p> <pre><code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$ conda create -n &lt;name&gt; python=3.10 ipykernel\n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ conda activate &lt;name&gt;\n# Now you will see your terminal became:\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$\n# install the packages using the commands provided above\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$ pip install -r requirements.txt\n# run your Python scripts\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$ python your_script.py\n\n# Go back to the base environment\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$ conda deactivate\n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ \n</code></pre>"},{"location":"software/conda/#run-your-job-on-the-cluster","title":"Run your job on the cluster","text":"<p>As indicated before, your <code>bash</code> script is very simple:</p> <pre><code>#!/bin/bash\n# run_python.sh\nset -e\n#SBATCH --account &lt;your account&gt;\n#other settings\n\npython your_script.py\n</code></pre> <p>Then choose your preferred environment, let's say an environment called <code>neuroimaging</code>, and type the following command:</p> <pre><code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$ conda activate neuroimaging\n(neuroimaging) [&lt;usr&gt;@bb-pg-login04 ~]$ sbatch run_python.sh\n# check your output:\n(neuroimaging) [&lt;usr&gt;@bb-pg-login04 ~]$ tail -f slurm-&lt;id&gt;.out \nProcessing subj_id: CC222555:  24%|\u2588\u2588\u258e       | 153/651 [08:42&lt;28:37,  3.45\n</code></pre>"},{"location":"software/containers/","title":"Containers","text":"<p>A container is a lightweight software package that contains both the software, and all of the required dependencies to run the contained software.</p> <p>BlueBEAR supports running analyses on containers using Apptainer. The BEAR Technical docs contain extensive tutorals.</p> <p>Note</p> <p>BlueBEAR does not directly support Docker as it requires administrator privileges to run. Apptainer is able to read and execute Docker images without admin rights. Apptainer is the successor to the Singularty project - please see this article for more information on the transition.</p>"},{"location":"software/containers/#downloading-a-container","title":"Downloading a Container","text":"<p>Docker has a wide selection of containers available to download. BEAR Technical docs contains some examples on how to download and execute a simple python container.</p> <p>The following <code>bash</code> code provides an example of how to download the fMRIPrep container, which includes a variety of neuroimaging software, including freesurfer, FSL, and ANTS. This can be executed on an interactive or batch job.</p> <pre><code>apptainer pull --name fMRIPrep.sif docker://nipreps/fmriprep:latest\n</code></pre> <p>and this version can be submitted as a cluster script.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account bagshaap-example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 10 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n#SBATCH --constraint icelake\n\nset -e\n\napptainer pull --name fMRIPrep.sif docker://nipreps/fmriprep:latest\n</code></pre> <p>Save the code above into a bash script called <code>create-container_fmriprep.sh</code> (make sure you update the <code>account</code> name on line 3 to a project that you have access to!) inside the directory where you would like to save the container file. This can then be submitted to the cluster by running <code>sbatch create-container_fmriprep.sh</code> in a terminal, or creating a job in the 'job composer'. This will take several minutes to run to completion with the <code>fmriprep</code> image.</p> <p>Once the job has completed, you should be able to find a file called <code>fMRIPrep.sif</code> in your working directory (alongside the cluster log files). This is the container file.</p>"},{"location":"software/containers/#running-software-using-a-container","title":"Running Software using a Container","text":"<p>The <code>apptainer exec</code> command is used to run the contained software. The following bash codes demonstrates how to run the FSL command <code>fslroi</code> contained within the fMRIPrep container.</p> <pre><code>#!/bin/bash\n#SBATCH --account bagshaap-example-project\n#SBATCH --qos bbdefault\n\nmodule purge; module load bluebear\n\napptainer exec fMRIPrep.sif fslroi --help\n</code></pre> <p>This code can be saved into a <code>bash</code> script called <code>check-container_fmriprep.sh</code> inside the same directory used above. You can run the job as above and check the cluster logfiles to see that the help text for the <code>fslroi</code> function was printed from within the container. This is a very simple example but can be adapted to run any command using the software inside the container.</p> <p>Let's break this down so we can build a more complex command. There are four parts to running a command with Apptainer. This is the core line:</p> <pre><code>apptainer exec fMRIPrep.sif fslroi --help\n</code></pre> <p>We could visualise this as</p> <pre><code>&lt;apptainer call&gt; &lt;apptainer command&gt; &lt;container image&gt; &lt;user command&gt;\n</code></pre> <p>where</p> <ul> <li><code>&lt;apptainer call&gt;</code> is simply <code>apptainer</code>. This specifies that we're using apptainer.</li> <li><code>&lt;apptainer command&gt;</code> is <code>exec</code>. This tells apptainer that we want to run a command.</li> <li><code>&lt;container image&gt;</code> is <code>fMRIPrep.sif</code>. This should point to an existing <code>.sif</code> file containing our container.</li> <li><code>&lt;user command&gt;</code> is <code>fslroi --help</code>. This is the command we actually want to run and the part that we'll most frequently be changing.</li> </ul> <p>So, to run a more complex command we'd simply need to update our <code>&lt;user command&gt;</code> to the function that we need to compute. You'll need to be sure that the appropriate command is included in the container with all its dependencies. The command can point to files and directories within RDS as usual. You can combine array jobs and container commands to run many parallel analyses all within equivalent containers.</p> <p>For a practical tutorial on running a container, please see the fMRIPrep example.</p>"},{"location":"software/matlab/","title":"MATLAB","text":"<p>Interactive MATLAB sessions run as a GUI App accessible from the BEAR Portal. Please follow the information on the BEAR Technical Docs to start up an interactive MATLAB session.</p> <p>Some parallelisation is available through parfor loops within single MATLAB but users looking for to run many individual MATLAB scripts in parallel are likely to want to use the Slurm job submissions. Examples of both are included below.</p>"},{"location":"software/matlab/#neuroimaging-toolboxes","title":"Neuroimaging toolboxes","text":"<p>Neuroimaging toolboxes can be added to the MATLAB path on BlueBEAR in the normal way. Toolboxes can be downloaded from the developer and stored on an RDS space. These folders can be added to the path within a MATLAB session using <code>addpath</code>.</p> <pre><code>addpath(genpath('/rds/q/quinna-example-project/code/fieldtrip'))\n</code></pre> <p>These pages include some specific examples using popular MATLAB toolboxes:</p> <ul> <li>Fieldtrip</li> <li>SPM</li> </ul>"},{"location":"software/matlab/#parallel-for-loop","title":"Parallel for-loop","text":"<p>Info</p> <p>Example contributed by Dagmar Fraser</p> <p>Simple parallelisation of a for-loop can be performed using parfor. This functionality is provided by MATLAB and enables faster processing of <code>for</code> loops simply by changing the syntax at the start to say <code>parfor</code> rather than <code>for</code>.</p> <p>The following MATLAB code performs some matrix calculations on simulated data. The inclusion of a <code>parfor</code> loop means that the code can take advantage of computers with multiple CPUs to accelerate processing.</p> <pre><code>tic\nn = 200;\nA= 500;\na = zeros(1,n);\nparfor i = 1:n\n    a(i) = max(abs(eig(rand(A))));\nend\ntoc\n</code></pre> <p>Run this a few times replacing <code>parfor</code> with <code>for</code> to get an idea of the time difference.</p> <p>Note</p> <p>Make sure you specify the appropriate number of cores when starting the MATLAB GUI App, you may not notice a substantial speed-up if you run MATLAB using the default of 4 cores. Do try to avoid asking for substantially more than you might need however - BlueBEAR is a shared resource!</p>"},{"location":"software/matlab/#submitting-matlab-jobs-with-parfor-to-bear","title":"Submitting MATLAB jobs with parfor to Bear","text":"<p>You can run this code in an interactive MATLAB session, or save it as a script that can be executed on the big cluster. If we save the code in the previous example as <code>parforDemo.m</code>, we can write a second 'submission' script to execute it on the cluster.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 8\n#SBATCH --time 5:0\n#SBATCH --qos bbshort\n#SBATCH --mail-type ALL\n\nset -e\n\nmodule purge\nmodule load bluebear\nmodule load MATLAB/2020a\n\nMATLAB -nodisplay -r parforDemo\n</code></pre> <p>If we save that second script as <code>RunMyCode.sh</code> it can be run using <code>sbatch RunMyCode.sh</code> on a terminal to send the job to the cluster. We can monitor the progress of the job using the 'Active Jobs' tab in BlueBEAR portal.</p> <p>The <code>ntasks</code> line specifies we are looking to use 8 cores. The last line contains the filename we are sending to MATLAB to execute.</p>"},{"location":"software/matlab/#submitting-multiple-matlab-jobs","title":"Submitting multiple MATLAB jobs","text":"<p>Info</p> <p>Example contributed by Katharina Deucker</p> <p>The previous example submits a single MATLAB job that uses <code>parfor</code> BlueBEAR, for larger analyses we may want to parallelise jobs across entire MATLAB instances. This can be done by submitting MATLAB jobs to BEAR using Slurm. The BEAR Technical Docs contain a simple example on submitting a MATLAB job to BEAR.</p> <p>For neuroimaging analyses, you'll generally need to organise your scripts so that each part that you want to parallelise runs from a single function that takes a single ID as an argument. Here is a specific example that runs a function <code>e1_fun_ICA</code> on each of 48 datasets.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --time 30:0\n#SBATCH --mem 50G\n#SBATCH --qos bbdefault\n#SBATCH --array=1-48\n\nset -eu\n\nmodule purge; module load bluebear\n\n# load the MATLAB version you need\nmodule load MATLAB/2019b\n\n# apply MATLAB script to each index in the array\n# (the MATLAB script is programmed such that the input ID is used as the subject ID)\nMATLAB -nodisplay -r \"run /rds/homes/d/dueckerk/startup.m, e1_fun_ICA(${SLURM_ARRAY_TASK_ID}), quit\"\n</code></pre>"},{"location":"software/python/","title":"Python","text":"<p>Python is a interactive programming language known for being flexible and (relatively) simple to use. A vast range of scientific applications have be built in and around Python.</p> <p>Some of the most common are:</p> <ul> <li>numpy: fundamental array computing in python</li> <li>scipy: fundamental algorithms in python</li> <li>pandas: manipulation and analysis of data tables</li> <li>scikit-learn: efficient tools for machine learning</li> </ul> <p>and many, many more. Many Python packages are distributed on PyPI.org</p>"},{"location":"software/python/#python-versions-on-bear","title":"Python versions on BEAR","text":"<p>Python versions up to 3.10 are supported as loadable modules on BEAR Apps. These can be loaded into a terminal session ready for use:</p> <pre><code>module load bear-apps/2022a\nmodule load Python/3.10.4-GCCcore-11.3.0\n</code></pre> <p>This will be sufficient to run a pure Python script inside that terminal session. Frequently we'll want to load a wider range of modules to use in the script. There are several ways to go about this.</p> <p>We could load these modules one at a time, ensuring that any versions relating to Python, FOSS or GCCCore all match each other.</p> <pre><code>module load bear-apps/2022a\nmodule load scikit-learn/1.1.2-foss-2022a\n</code></pre> <p>Note</p> <p>Modules will load any relevant dependencies at the same time, so loading <code>scikit-learn</code> will also load the relevant Python version into the session. It is best to trust the dependencies built into the <code>module load</code> system and only define the minimum necessary modules in your session.</p> <p>Or we can load a bundle of applications. The <code>SciPy-bundle</code> includes a bunch of packages including <code>numpy</code>, <code>scipy</code> and <code>pandas</code>.</p> <pre><code>module load bear-apps/2022a\nmodule load SciPy-bundle/2022.05-foss-2022a\n</code></pre> <p>iPython is a powerful Python console that you can use for interactive sessions in the terminal.</p> <pre><code>module load bear-apps/2022a\nmodule load SciPy-bundle/2022.05-foss-2022a\nmodule load matplotlib/3.5.2-foss-2022a\nmodule load IPython/8.5.0-GCCcore-11.3.0\n</code></pre> <p>You can then start an iPython session from a terminal:</p> <pre><code>ipython --pylab=tk\n</code></pre> <p>and start running some Python code using your loaded libraries:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.random.randn(10,)/2 + x\n\nplt.figure()\nplt.plot(x, y, 'o')\nplt.show()\n</code></pre> <p>You can also save some Python code into a file and run it on the command line (this is very useful for running jobs on the cluster later...). If we save the code above into a file called <code>my_plot.py</code> - we can run it in the terminal using:</p> <pre><code>python my_plot.py\n</code></pre>"},{"location":"software/python/#submitting-python-jobs-to-the-cluster","title":"Submitting Python jobs to the cluster","text":"<p>We need to prepare two things to run Python jobs on the BlueBEAR cluster: we need an executable Python script to run the analysis and a bash script to prepare an environment and actually run our code.</p> <p>Let's make a simple example. The following script creates and saves out a simple scatter plot of some random data.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.random.randn(10,)/2 + x\n\nplt.figure()\nplt.plot(x, y, 'o')\nplt.title('This ran on the cluster!')\nplt.xlabel('Variable 1')\nplt.ylabel('Variable 2')\nfor tag in ['top', 'right']:\n    plt.gca().spines[tag].set_visible(False)\nplt.grid(True)\nplt.savefig('my_cluster_figure.png')\n</code></pre> <p>We can save this script as <code>quick_python_plot.py</code>. Next, we need a <code>bash/slurm</code> script to submit and run our Python code.</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n\nmodule purge; module load bluebear\n\nmodule load bear-apps/2022a\nmodule load SciPy-bundle/2022.05-foss-2022a\nmodule load matplotlib/3.5.2-foss-2022a\n\npython quick_python_plot.py\n</code></pre> <p>We can save this as <code>submit_quick_python_plot.sh</code> in a directory next to our Python code (Remember to update the projecet on line 2 to a BEAR project that you can access!).</p> <pre><code>sbatch submit_quick_python_plot.sh\n</code></pre> <p>You can monitor the progress of your job in the active jobs tracker on BEAR portal. Once it has finished you should find a nice figure saved in your directory.</p> <p></p>"},{"location":"software/python/#jupyterlab","title":"JupyterLab","text":"<p>Interactive python notebooks are available to run as a JupyterLab GUI App through the BEAR Portal. The pre-installed Python modules can be loaded as modules in the notebook session.</p> <p>Only the pre-installed modules available in BEAR Apps are installable in the JupyterLab GUI App.</p>"},{"location":"software/python/#virtual-environments","title":"Virtual environments","text":"<p>More involved analyses may required dependencies or package versions that aren't available on BEAR Apps. The next optionn for these analysis is to use virtual environments as described on the BEAR Technical Docs.</p> <p>The following <code>bash</code> script (adapted from the main docs) loads the standard BEAR modules for MNE-Python, creates a virtual environment and then installs the EMD package with <code>pip</code>:</p> <pre><code>#!/bin/bash\nset -e\n\n# Load our core modules from BEAR\nmodule purge; module load bluebear\nmodule load bear-apps/2021b\nmodule load Python/3.9.6-GCCcore-11.2.0\n\n# Prepare path locations and name for virtual environment\nexport VENV_DIR=\"${HOME}/virtual-environments\"\nexport VENV_PATH=\"${VENV_DIR}/my-virtual-env-${BB_CPU}\"\n\n# Create a master venv directory if necessary\nmkdir -p ${VENV_DIR}\n\n# Check if virtual environment exists and create it if not\nif [[ ! -d ${VENV_PATH} ]]; then\n    python3 -m venv --system-site-packages ${VENV_PATH}\nfi\n\n# Activate the virtual environment\nsource ${VENV_PATH}/bin/activate\n\n# Perform any required pip installations. For reasons of consistency we would recommend\n# that you define the version of the Python module \u2013 this will also ensure that if the\n# module is already installed in the virtual environment it won't be modified.\npip install emd==0.5.4\n</code></pre> <p>You can save this into a shell script such as <code>init_myenv.sh</code> and run it using <code>source init_myenv.sh</code> to create the environment. You can now run <code>init_myenv.sh</code> when opening a new terminal to initialise an environment before running scripts or interactive sessions. The code above is all you need for this option, you can add or change the dependencies in the script as you need.</p>"},{"location":"software/python/#python-on-the-cluster","title":"Python on the cluster","text":"<p>You can also adapt the script to submit jobs to the cluster. For this, we'll need to add the appropriate <code>slurm</code> commands to the start of the script and add a line running our analysis to the end. That might look something like this:</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n\nset -e\n\n# Load our core modules from BEAR\nmodule purge; module load bluebear\nmodule load bear-apps/2021b\nmodule load Python/3.9.6-GCCcore-11.2.0\n\n# Prepare path locations and name for virtual environment\nexport VENV_DIR=\"${HOME}/virtual-environments\"\nexport VENV_PATH=\"${VENV_DIR}/my-virtual-env-${BB_CPU}\"\n\n# Create a master venv directory if necessary\nmkdir -p ${VENV_DIR}\n\n# Check if virtual environment exists and create it if not\nif [[ ! -d ${VENV_PATH} ]]; then\n    python3 -m venv --system-site-packages ${VENV_PATH}\nfi\n\n# Activate the virtual environment\nsource ${VENV_PATH}/bin/activate\n\n# Perform any required pip installations. For reasons of consistency we would recommend\n# that you define the version of the Python module \u2013 this will also ensure that if the\n# module is already installed in the virtual environment it won't be modified.\npip install emd==0.5.4\n\n# Python script to be run.\npython emd_example.py\n</code></pre> <p>Note the additional <code>SBATCH</code> options at the start and the <code>python emd_example.py</code> at the end. We can save this script as 'submit_emd_example.sh`.</p> <p>We'll need a Python Let's use this as an example. We can save the following script as <code>emd_example.py</code> on RDS.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport emd\n\n# Define and simulate a simple signal\npeak_freq = 15\nsample_rate = 256\nseconds = 10\nnoise_std = .4\nx = emd.simulate.ar_oscillator(peak_freq, sample_rate, seconds,\n                               noise_std=noise_std, random_seed=42, r=.96)[:, 0]\nx = x*1e-4\nt = np.linspace(0, seconds, seconds*sample_rate)\n\n# Run a mask sift\nimf = emd.sift.mask_sift(x, max_imfs=5)\n\nfig = plt.figure()\nemd.plotting.plot_imfs(imf[:sample_rate*3, :], fig=fig)\nfig.savefig('my-emd-example.png')\n</code></pre> <p>Now, we can submit our job to the cluster.</p> <pre><code>sbatch submit_emd_example.sh\n</code></pre> <p>You can monitor the progress of your job in the active jobs tracker on BEAR portal. Once it has finished you should find a nice new figure saved in your working directory.</p> <p></p>"}]}