{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CHBH Computing on Bear","text":"<p>CHBH-on-BEAR extends the Bear Technical Documentation pages tutorials and examples for neuroimaging analyses to run on BlueBEAR.</p> <p>Sections are organised by core neuroimaging software packages. Practical examples and explanations are provided where possible.</p> <p>Check out the links in the sidebar to get started. Try 'Getting Started' if you're a new user or jump into a specific software if you already have some experience with BEAR computing.</p> <p>Note</p> <p>This website is intended for current staff and students at the Centre for Human Brain Health and University of Birmingham. It is not likely to be useful for anyone else...</p>"},{"location":"#contributors","title":"Contributors","text":"<p>Many thanks to our contributors!</p> <sub>ajquinn</sub>\ud83d\udea7 \ud83d\udd8b <sub>James Carpenter</sub>\ud83d\udea7 \ud83d\udd8b <sub>Brandon Ingram</sub>\ud83d\udd8b <sub>Arkady Konovalov</sub>\ud83d\udd8b <sub>Ben Griffiths</sub>\ud83d\udd8b <sub>Tara</sub>\ud83d\udd8b <sub>Dagmar S Fraser</sub>\ud83d\udd8b <sub>Jianzhang Ni</sub>\ud83d\udd8b <sub>Katharina</sub>\ud83d\udd8b"},{"location":"#contributing","title":"Contributing","text":"<p>This page is a work-in-progress and contributions are very welcome! Please email Andrew or make some changes directly on the CHBH-on-BEAR github page.</p>"},{"location":"bear/","title":"Getting started on BEAR","text":"<p>This page collects tutorials and examples for neuroimaging analyses to run on BlueBEAR. This is intended to extend the main Bear Technical Documentation pages.</p> <p>Here we provide a set of links, tips and tricks for getting started. Mostly linking out to other places.</p>"},{"location":"bear/#step-0-linux","title":"Step 0: Linux","text":"<p>Bear provide a Introduction to Linux guide. Many computing services on bear rely on linux. There are in-person workshops and an online canvas courses available on this page.</p>"},{"location":"bear/#step-1-bluebear","title":"Step 1: BlueBEAR","text":"<p>Bear also provide a Introduction to BlueBEAR course. There are in person workshops and an online canvas course.</p>"},{"location":"bear/#step-2-rds-projects","title":"Step 2: RDS Projects","text":"<p>You'll need to be a member of a Bear project and have a Bear linux account to use BlueBEAR. Your PI and lab can help with this. A detailed guide for accessing BEAR is provided on the technical docs.</p>"},{"location":"bear/#step-3-bear-portal","title":"Step 3: Bear Portal","text":"<p>BEAR Portal  provides web-based access to a range of BEAR services, such as JupyterLab, RStudio, and various other GUI applications. BEAR Portal is only available on campus or using the University Remote Access Service.</p>"},{"location":"bear/#step-4-launching-interactive-sessions","title":"Step 4: Launching interactive sessions","text":"<p>From BEAR Portal there are three options for launching an interactive analysis session.</p> <ul> <li> <p>Some software packages have GUI Apps  installed on BlueBEAR that can be launched from the Bear Portal - the main example for neuroimaging analysis is Matlab.</p> </li> <li> <p>JupyterLab and RStudio are installed as standalone apps that can be launched from BEAR Portal. (ote that only packages installed on Bear Apps are available to load in JupyterLab).</p> </li> <li> <p>A complete Linux Desktop can be launched as the BlueBEAR GUI. BlueBEAR GUI is effectively a blank-slate linux desktop, into which you can load the modules for various applications, specify environment variables etc. by using the built-in Terminal client (see image below), and then ultimately launch the interface for the application that you require.</p> </li> </ul>"},{"location":"bear/#step-5-running-cluster-jobs-with-slurm","title":"Step 5: Running cluster jobs with Slurm","text":"<p>There are two ways to submit a cluster job - the bluebear terminal or the Bear Portal</p> <p>https://docs.bear.bham.ac.uk/bluebear/jobs/ https://docs.bear.bham.ac.uk/portal/jobs/</p>"},{"location":"bear/#step-6-neuroimaging-analysis-software-on-bear","title":"Step 6: Neuroimaging analysis software on Bear","text":"<p>The following software is available on BlueBEAR.</p> Toolbox GUI App Bear Apps Modules Notes FSL N/A Bear Apps FSL Pip install modules via venv Python JupyterLab Bear Apps Python Bear Apps or pip/venv MNE-Python JupyterLab Bear Apps MNE Bear Apps or pip/venv Matlab MatLab Bear Apps MatLab Fieldtrip MatLab None Load within MatLab script EEGLab MatLab None Load within MatLab script SPM MatLab None Load within MatLab script R Rstudio Bear Apps R Freesurfer N/A Bear Apps Freesurfer"},{"location":"faqs/","title":"Frequently Asked Questions","text":"<p>Is something incorrect, out-of-date or missing from this page? Open an issue on github and let us know.</p>"},{"location":"faqs/#where-can-i-ask-for-help","title":"Where can I ask for help?","text":"<p>You can ask for informal help via the CHBH Code, Computing and Open Science teams page. More formal support is availabl by raising a support ticket with IT.</p>"},{"location":"faqs/#general","title":"General","text":"<p>What is a 'core'? is it the same as a 'CPU'?</p> <p>Core and CPU refer to a Central Processing Unit. The terms are sometimes used interchangeably.</p> <p>What does it mean if I use '4 cores'?</p> <p>Each core can run a single computation at a time. If we have 4 cores, we can run 4 different things can happen at once. And 50 cores means 50 jobs could be run in parallel.</p> <p> Is castles the same as rds space?</p> <p>CaStLeS (Compute and Storage for Life Sciences) resources (both compute and storage) are a constituent part of BEAR Cloud, BlueBEAR and the wider BEAR infrastructure. They are reserved exclusively for the use of research groups carrying out research in the life sciences.</p> <p>In CHBH, 'Castles' has often referred to Virtual Machine environments used for analyses. These are been phased out in favour of the BEAR GUI.</p> <p> What about code sharing? Not everybody has a Bluebear. Will people be able to run my code if I\u2019ve written it for use on this platform?</p> <ul> <li>Think in terms of \u2018develop\u2019 vs \u2018rationalise\u2019. The second part is where parallelising becomes a bi advantage. It\u2019s also the place you think about code sharing.</li> <li>There are clever ways to make things very shareable agnostic to infrastructure. E.g., putting everything into a docker container and running \u2018container jobs\u2019. This has the disadvantage that you lose bear\u2019s optimisation (all modules have been optimised so may run a lot faster). But you gain control. Tradeoff</li> </ul>"},{"location":"faqs/#bear-portal","title":"BEAR Portal","text":"<p>What is included in the 'Number of hours' when starting a service on BEAR portal?</p> <p>The \u2018number of hours\u2019 means that the GUI is required for N hours, not that the whole analysis needs to last N hours.  The 8 hour limit is for interactive jobs or apps started from the BEAR portal. The time limit on the main cluster is 10 days.</p> <p>What does \u2018working directory\u2019 mean when starting VS Code?</p> <p>This changes root dir that VScode opens in, though it can only operate within your RDS home space, not a group space.</p> <p>If you want to edit files in a group space using VS Code, you'll need to create a 'Symbolic Link' that provides a path to the group from within your home space. For example, to link a project to a home space you can run</p> <pre><code>ln -s /rds/groups/q/quinna-example-project /rds/homes/q/quinna/\n</code></pre> <p>There will now be a link directory in <code>/rds/homes/q/quinna/quinna-example-project</code> that you can use to see the files in the group space. Note that this is a 'link' not a copy.</p> <p>Do I NEED to run a VScode interactive session - couldn\u2019t I use my own local code editor?</p> <p>If you\u2019ve mounted your RDS drive, you could also use a local text editor, any one that you like. But, if you\u2019re running things locally on BEAR, an advantage is that you don\u2019t need to copy files back and forth which adds a step and can be cumbersome.</p> <p>Does VScode track changes to code?</p> <p>VScode doesn\u2019t track changes itself but VScode should be able to show you which files have been modified if you are reading files within an existing git repository. Git should be used to handle version tracking.</p>"},{"location":"faqs/#matlab","title":"Matlab","text":"<p>What situations would it make sense to use parfor in matlab?</p> <p>The most common neuroimaging use-case for <code>parfor</code> is looping over participants for first-level analyses. For example, when you want to run an identical set of steps on every file in a datasets.</p> <p>Other examples might include computing something from every voxel/channel/frequency in a single dataset, or running non-parametric permutatation stats.</p> <p>What if you asked for 50 CPUs and DIDN\u2019T use a parfor? Would it still be faster?</p> <p>Not necessarily. It depends on how the person wrote the code, occasionally a toolbox might know how to parallelise a process internally but this is unlikely by default. It is possible to ask for 50 cores and then accidentally run an analysis on only one of them.w</p> <p>Can I write a startup.m script to do things like adding paths to tools I use all the time, so MATLAB will automatically execute this when it starts up?</p> <p>Yes. A matlab <code>startup.m</code> script in your local home directory will run as normal when opening matlab.</p> <p>For people who are already using matlab/rdesktop on their own computer, what are the key steps to get the best optimisation as you\u2019ve described when they switch to the cluster?</p> <p>Don\u2019t optimise too early. Look at your data first - lots of visualisation etc which is more interactive. But when you\u2019ve got your pipeline hardened, that\u2019s when you want to do all this stuff. Or when you\u2019ve got a pipeline but want to change something and re-run.</p>"},{"location":"getting_started/","title":"Getting started on BEAR","text":"<p>This page collects tutorials and examples for neuroimaging analyses to run on BlueBEAR. This is intended to extend the main Bear Technical Documentation pages.</p> <p>Here we provide a set of links, tips and tricks for getting started. Mostly linking out to other places.</p>"},{"location":"getting_started/#step-0-linux","title":"Step 0: Linux","text":"<p>Bear provide a Introduction to Linux guide. Many computing services on bear rely on linux. There are in-person workshops and an online canvas courses available on this page.</p>"},{"location":"getting_started/#step-1-bluebear","title":"Step 1: BlueBEAR","text":"<p>Bear also provide a Introduction to BlueBEAR course. There are in person workshops and an online canvas course.</p>"},{"location":"getting_started/#step-2-rds-projects","title":"Step 2: RDS Projects","text":"<p>You'll need to be a member of a Bear project and have a Bear linux account to use BlueBEAR. Your PI and lab can help with this. A detailed guide for accessing BEAR is provided on the technical docs.</p>"},{"location":"getting_started/#step-3-bear-portal","title":"Step 3: Bear Portal","text":"<p>BEAR Portal  provides web-based access to a range of BEAR services, such as JupyterLab, RStudio, and various other GUI applications. BEAR Portal is only available on campus or using the University Remote Access Service.</p>"},{"location":"getting_started/#step-4-launching-interactive-sessions","title":"Step 4: Launching interactive sessions","text":"<p>From BEAR Portal there are three options for launching an interactive analysis session.</p> <ul> <li> <p>Some software packages have GUI Apps  installed on BlueBEAR that can be launched from the Bear Portal - the main example for neuroimaging analysis is Matlab.</p> </li> <li> <p>JupyterLab and RStudio are installed as standalone apps that can be launched from BEAR Portal. (ote that only packages installed on Bear Apps are available to load in JupyterLab).</p> </li> <li> <p>A complete Linux Desktop can be launched as the BlueBEAR GUI. BlueBEAR GUI is effectively a blank-slate linux desktop, into which you can load the modules for various applications, specify environment variables etc. by using the built-in Terminal client (see image below), and then ultimately launch the interface for the application that you require.</p> </li> </ul>"},{"location":"getting_started/#step-5-running-cluster-jobs-with-slurm","title":"Step 5: Running cluster jobs with Slurm","text":"<p>There are two ways to submit a cluster job - the bluebear terminal and the job submission page on Bear Portal</p> <p>BlueBEAR Terminal Once you have prepared a submission script, you can go to that location in the BEAR-Portal file browser and click 'Open in termnal' in the top. This will open a terminal session from which you can submit, monitor and (optionally) cancel your job using <code>sbatch</code>, <code>squeue</code> and <code>scontrol</code>.</p> <p>Note</p> <p>These terminal sessions are ONLY intended for submitting and monitoring cluster jobs - not for active analyses. This should be carried out on BEAR GUI or similiar.</p> <p>BlueBEAR Job Composer this is a GUI page which helps you to write a new job to submit to blue-bear using the job composer.</p>"},{"location":"home/","title":"CHBH Computing on Bear","text":"<p>This page collects tutorials and examples for neuroimaging analyses to run on BlueBEAR. This is intended to extend the main Bear Technical Documentation pages.</p>"},{"location":"home/#getting-started","title":"Getting Started","text":""},{"location":"home/#contributing","title":"Contributing","text":"<p>This page is a work-in-progress! Contributions from github PRs or by emailing Andrew...</p>"},{"location":"neuroimaging_software/","title":"Index of Neuroimaging Software","text":"<p>This is an incomplete list of the neuroimaging software that is available on BlueBEAR. The links in the 'Toolbox' column follow to CHBH-on-BEAR examples and the 'Bear Apps' links go to the module specifications provided by BEAR.</p> Toolbox GUI App Bear Apps Modules Notes FSL N/A Bear Apps FSL Pip install modules via venv Python JupyterLab Bear Apps Python Bear Apps or pip/venv MNE-Python JupyterLab Bear Apps MNE Bear Apps or pip/venv Matlab MatLab Bear Apps MatLab Fieldtrip MatLab None Load within MatLab script EEGLab MatLab None Load within MatLab script SPM MatLab None Load within MatLab script R Rstudio Bear Apps R Freesurfer N/A Bear Apps Freesurfer"},{"location":"R/R/","title":"R for statistical computing","text":"<p>The R project is a free software environment for statistical computing and graphics.</p>"},{"location":"R/R/#r-versions","title":"R Versions","text":"<p>Bear Apps has several versions of MNE-Python as loadable modules.</p>"},{"location":"R/R/#r-studio-gui-app","title":"R-Studio GUI App","text":"<p>RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging, and workspace management.</p> <p>You can open an interactive RStudio session through the Bear Portal. The pre-installed R versions can be loaded.</p>"},{"location":"R/R/#neuroimaging-specific-r-packages","title":"Neuroimaging specific R packages","text":"<p>Here is a list of R packages commonly used for neuroimaging analysis.</p>"},{"location":"R/R/#fslr","title":"FSLR","text":"<p>Wrapper functions that interface with 'FSL' , a powerful and commonly-used 'neuroimaging' software, using system commands.</p>"},{"location":"R/R/#r-example-for-bear","title":"R Example for BEAR","text":"<pre><code>knitr::opts_chunk$set(echo = TRUE)\nlibrary(ggplot2)\n\n# Simulate some data\ntime     &lt;- 0:99\nset.seed(1)\nnoise    &lt;- rnorm(100)\ndisorder &lt;- time * 4 + 100 + noise * 20\ndis_df   &lt;- data.frame(time, disorder)\n\n# Create a plot\nggplot(dis_df, aes(x = time, y = disorder)) + geom_point() +\n  geom_smooth(method = \"lm\")\n\n# Fit model\nlm_fit &lt;- lm(disorder ~ time, dis_df)\nsummary(lm_fit)\n</code></pre>"},{"location":"containers/containers/","title":"Containers","text":"<p>A container is a lightweight software package that contains both the software, and all of the required dependencies to run the contained software.</p> <p>BlueBEAR supports running analyses on containers using Apptainer. The Bear Technical docs contain extensive tutorals</p> <p>Note</p> <p>BlueBEAR does not directly support Docker as it requires administratr privalages to run. Apptainer is able to read and execute Docker images without admin rights. Apptainer is the successor to the Singularty project - please see this article for more information on the transition.</p>"},{"location":"containers/containers/#downloading-a-container","title":"Downloading a Container","text":"<p>Docker has a wide selection of containers available to download. Bear Technical docs contains some examples on how to download and execute a simple python container.</p> <p>The following bash code provides an example of how to download the fMRIPrep container, which includes a variety of neuroimaging software, including freesurfer, FSL, and ANTS. This can be executed on an interactive or batch job.</p> <pre><code>apptainer pull --name fMRIPrep.sif docker://nipreps/fmriprep:latest\n</code></pre> <p>and this version can be submitted as a cluster script.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account bagshaap-example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 10 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n#SBATCH --constraint icelake\n\nset -e\n\napptainer pull --name fMRIPrep.sif docker://nipreps/fmriprep:latest\n</code></pre> <p>save the code above into a bash script called <code>create-container_fmriprep.sh</code> (make sure you update the <code>account</code> name on line 3 to a project that you have access to!) inside the directory where you would like to save the container file. This can then be submitted to the cluster by running <code>sbatch create-container_fmriprep.sh</code> in a terminal, or creating a job in the 'job composer'. This will take several minutes to run to completion with the <code>fmriprep</code> image.</p> <p>Once the job has completed, you should be able to find a file called <code>fMRIPrep.sif</code> in your working directory (alongside the cluster log files). This is the container file.</p>"},{"location":"containers/containers/#running-software-using-a-container","title":"Running Software using a Container","text":"<p>The <code>apptainer exec</code> command is used to run the contained software. The following bash codes demonstrates how to run the FSL command <code>fslroi</code> contained within the fMRIPrep container.</p> <pre><code>#!/bin/bash\n#SBATCH --account bagshaap-example-project\n#SBATCH --qos bbdefault\n\nmodule purge; module load bluebear\n\napptainer exec fMRIPrep.sif fslroi --help\n</code></pre> <p>This code can be saved into a bash script called <code>check-container_fmriprep.sh</code> inside the same directory used above. You can run the job as above and check the cluster logfiles to see that the help text for the <code>fslroi</code> function was printed from within the container. This is a very simple example but can be adapted to run any command using the software inside the container.</p> <p>Let's break this down so we can build a more complex command. There are four parts to running a command with Apptainer. This is the core line:</p> <pre><code>apptainer exec fMRIPrep.sif fslroi --help\n</code></pre> <p>We could visualise this as</p> <pre><code>&lt;apptainer call&gt; &lt;apptainer command&gt; &lt;container image&gt; &lt;user command&gt;\n</code></pre> <p>where</p> <ul> <li><code>&lt;apptainer call&gt;</code> is simply <code>apptainer</code>. This specifies that we're using apptainer.</li> <li><code>&lt;apptainer command&gt;</code> is <code>exec</code>. This tells apptainer that we want to run a command.</li> <li><code>&lt;container image&gt;</code> is <code>fMRIPrep.sif</code>. This should point to an existing <code>.sif</code> file containing our container.</li> <li><code>&lt;user command&gt;</code> is <code>fslroi --help</code>. This is the command we actually want to run and the part that we'll most frequently be changing.</li> </ul> <p>So, to run a more complex command we'd simply need to update our <code>&lt;user command&gt;</code> to the function that we need to compute. You'll need to be sure that the appropriate command is included in the container with all its dependencies. The command can point to files and directories within RDS as usual. You can combine array jobs and container commands to run many parallel analyses all within equivalent containers.</p>"},{"location":"fmriprep/fmriprep/","title":"fMRIPrep","text":"<p>fMRIPrep is an open-source tool for preprocessing functional MRI (fMRI) data. It automates the early stages of MRI data preprocessing, including motion correction, susceptibility distortion correction, and slice-timing correction, providing a standardised and reproducible analysis pipeline. Unlike other preprocessing pipelines, fMRIPrep utilizes functions from many popular neuroimaging tools, including FSL, FreeSurfer, and AFNI, ensuring that each step in the pipeline uses the most reliable and validated methods available, resulting in higher-quality outputs.</p>"},{"location":"fmriprep/fmriprep/#downloading-fmriprep","title":"Downloading fMRIPrep","text":"<p>The simplest way of running fMRIPrep is using a container. Instructions for downloading the fMRIPrep container are detailed here.</p>"},{"location":"fmriprep/fmriprep/#running-fmriprep","title":"Running fMRIPrep","text":"<p>In order to run fMRIPrep, your data must first be organised according the brain imaging data structure BIDS guidlines. Once organised into BIDS, the default fMRIPrep can be run using the following script:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem 18G\n\nbids_directory=camcan_bids/\noutput_directory=camcan_fmriprep/\n\napptainer run fmriprep_24_1_1.sif ${bids_directory} ${output_directory} participant -w work/ --participant-label 01 --fs-license-file ~/license.txt \n</code></pre> <p>This script can then be submitted using the following command:</p> <pre><code>sbatch fmriprep.sh\n</code></pre> <p>Descriptions of the variables and arguments:</p> Variables / Arguments Description <code>bids_dir</code> First positional argument. Path to the BIDS-formatted dataset. <code>analysis_level</code> Second positional argument. Should be set to \"participant\" (required). <code>output_dir</code> Third positional argument. Sets the output directory. <code>-w</code> Specifies the working directory to store intermediate files during preprocessing. <code>--participant-label</code> Specifies the BIDS subject ID, enabling fMRIPrep to run on a single subject or a subset of subjects. <code>--fs-license-file</code> Path to the FreeSurfer license file (see FreeSurfer). By default, fMRIPrep uses FreeSurfer for anatomical co-registration. <code>--nprocs</code> Number of CPU cores to use for processing. <code>--mem</code> Amount of memory available in GB. <p>These arguments represent only a selection of the available options. A complete list of arguments can be found here, allowing you to fully customise the preprocessing pipeline to your specific needs.</p>"},{"location":"fmriprep/fmriprep/#running-fmriprep-across-all-subjects","title":"Running fMRIPrep Across all Subjects","text":"<p>It is also possible to run all subjects within a BIDS directory by using the following modified script:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem 18G\n#SBATCH --array=0-20 # Number of subjects in the BIDS directory.\n\nSUBJECTS=($(find camcan_bids -maxdepth 1 -type d -name 'sub-*' | sort | xargs -n 1 basename))\nSUBJECT_ID=${SUBJECTS[$SLURM_ARRAY_TASK_ID]}\nbids_directory=camcan_bids/\noutput_directory=camcan_fmriprep/\n\napptainer run fmriprep_24_1_1.sif ${bids_directory} ${output_directory} participant -w work/ --participant-label ${SUBJECT_ID} --fs-license-file ~/license.txt \n</code></pre> <p>The number at the end fo the \"#SBATCH --array=0-20\" should be replaced with the number of subjects within the BIDS directory.</p>"},{"location":"freesurfer/freesurfer/","title":"FreeSurfer","text":"<p>FreeSurfer is an open-source package for the analysis and visualization of structural, functional, and diffusion neuroimaging data from cross-sectional and longitudinal studies.</p>"},{"location":"freesurfer/freesurfer/#freesurfer-license","title":"FreeSurfer License","text":"<p>FreeSurfer requires a license registration key in order to be used. This can be obtained from here. Once downloaded, the file should be uploaded to your home directory on Bear. This can be done using \"Files\" tab on the BlueBEAR portal, or using file transfer software, such as WinSCP, or FileZilla. </p>"},{"location":"freesurfer/freesurfer/#running-recon-all","title":"Running recon-all","text":"<p>The <code>recon-all</code> command performs all, or any part of, the FreeSurfer cortical reconstruction process. The outputs of <code>recon-all</code> can be used to define the surfaces required for the boundary estimate model (BEM) required when performing source reconstruction on M/EEG data. The function can be run via BlueBEAR using the example script (recon_all.sh) below:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem-per-cpu 2\n\nmodule purge\nmodule load bear-apps/2019a/live\nmodule load FreeSurfer/6.0.1-centos6_x86_64\n\nexport FS_LICENSE=${HOME}/license.txt\nexport SUBJECTS_DIR=/rds/projects/b/bagshaap-eeg-fmri-hmm/fs_outputs\n\nrecon-all -s sub-01 -i /rds/projects/b/bagshaap-eeg-fmri-hmm/T1_vol_v1_5.nii.gz \\\n-all \\ \n-log logfile \\ \n-all \\\n-parallel -openmp 4\n</code></pre> <p>This script can then be submitted using the following command:</p> <pre><code>sbatch recon_all.sh\n</code></pre> <p>Descriptions of the variables and arguments:</p> Variables / Arguments Description <code>FS_LICENSE</code> Sets the path to the FreeSurfer license file. <code>SUBJECTS_DIR</code> Sets the output directory for the analysis. <code>-s</code> Sets the name of the output folder. <code>-i</code> Specifies the full path to the T1-weighted MRI image. <code>-all</code> Instructs FreeSurfer to run all processing steps. <code>-log</code> Creates a log file named \"logfile\" upon completion of the processing. <code>-parallel</code> Enables parallel processing in FreeSurfer. <code>-openmp</code> Defines the number of CPU cores available for parallel processing. <p>For the above script to work for you, several of the variables and arguments need to be changed to match your filenames and directories. Specifically, the <code>SUBJECT_DIR</code> variable needs to be changed to the path where you want the outputs to be saved, the <code>-s</code> argument needs to be changed to the desired name of the output folder, and the <code>-i</code> argument needs to be changed to a path to your T1 file. </p>"},{"location":"freesurfer/freesurfer/#running-recon-all-on-multiple-subjects","title":"Running recon-all on Multiple Subjects","text":"<p>Alternatively, if you need to run recon-all for multiple subjects at once, for example, on an entire BIDS dataset, it is possible to submit all jobs using the below script (recon_all_bids.sh):</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem-per-cpu 2\n#SBATCH --array=1-20 # 20 represents the total number of subjects\n\nmodule purge\nmodule load bear-apps/2019a/live\nmodule load FreeSurfer/6.0.1-centos6_x86_64\n\nexport FS_LICENSE=${HOME}/license.txt\nexport SUBJECTS_DIR=/rds/projects/b/bagshaap-eeg-fmri-hmm/fs_outputs\n\nsubject_id_number=$(printf \"%02d\" ${SLURM_ARRAY_TASK_ID})\n\nrecon-all -s sub-${subject_id_number} \\\n-i /rds/projects/b/bagshaap-eeg-fmri-hmm/bids_dataset/sub-${}/anat/sub-${subject_id_number}_T1w.nii.gz  \\\n-all \\ \n-log logfile \\ \n-all \\\n-parallel -openmp 4\n</code></pre>"},{"location":"freesurfer/freesurfer/#running-freesurfer-in-a-container","title":"Running FreeSurfer in a Container","text":"<p>Running FreeSurfer within a container allows for greater control over the software versions and improves the reproducibility of the analysis. FreeSurfer containers are available on dockerhub, or can be created using NeuroDocker (see the Containers page for details on downloading containers). In the example below we assume a container named 'freesurfer.sif' has been downloaded:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --qos bbdefault\n#SBATCH --time 1440\n#SBATCH --ntasks 4\n#SBATCH --mem-per-cpu 2\n\nFS_LICENSE=${HOME}/license.txt\nSUBJECTS_DIR=/rds/projects/b/bagshaap-eeg-fmri-hmm/Projects/Visual_Response_Variability/fs_outputs\n\napptainer exec --env FS_LICENSE=${FS_LICENSE} --env SUBJECTS_DIR=${SUBJECTS_DIR} \\\nfreesurfer.sif \\\nrecon-all -s sub-01 -i /rds/projects/b/bagshaap-eeg-fmri-hmm/T1_vol_v1_5.nii.gz \\\n-log logfile \\ \n-all \\\n-parallel -openmp 4\n</code></pre>"},{"location":"fsl/fsl/","title":"FSL","text":"<p>FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.</p>"},{"location":"fsl/fsl/#fsl-modules","title":"FSL Modules","text":"<p>A range of installed FSL versions are available as modules on Bear Apps.</p>"},{"location":"fsl/fsl/#bear-portal-gui","title":"Bear Portal GUI","text":"<p>The following code snippet can be executed in a terminal from within the Bear Portal GUI. It will load a pre-installed FSL version into the terminal where is can be used as normal.</p> <pre><code>module load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n</code></pre> <p>We can then use FSL command line functions as normal.</p> <pre><code>fsl_anat --help\n</code></pre> <p>or open the FSL GUI:</p> <pre><code>fsl\n</code></pre>"},{"location":"fsl/fsl/#fsleyes-on-bear-portal-gui","title":"FSLEyes on Bear Portal GUI","text":"<p>FSLEyes is the MRI volume visualisation tool provided and maintained by the FSL team. This runs well in Bear GUI and can be added to the local environment by adding the following module. (See the FSLEyes page on Bear Apps for all available versions). </p> <pre><code>module load FSLeyes/1.3.3-foss-2021a\n</code></pre> <p>Once the module has loaded you can run FSLEyes from the terminal as normal.</p> <pre><code>fsleyes\n</code></pre> <p>Info</p> <p>Note that it is currently not possible to have both FSL and FSLEyes in the environment in the same terminal. Until this is fixed, please use two separate terminal sessions, one for FSL and one for FSLEyes.</p>"},{"location":"fsl/fsl/#fsl-on-the-cluster","title":"FSL on the cluster","text":"<p>We can also run FSL jobs on the cluster using job scripts. The following can be saved as <code>run_fsl_bet.sh</code> and submitted to the cluster using <code>sbatch</code>. This will run a brain extraction on a single datafile.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account quinna-camcan\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 5 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n\nmodule purge\nmodule load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n\nset -e\n\nbet subject1.nii.gz subject1_brain.nii.gz\n</code></pre> <p>If we have many datafiles to run BET on, we can extend our script into an array job. This is slurm script that actually creates many jobs that can be run in parallel. Here we add the <code>#SBATCH --array=1-48</code> line to our script to tell it that we want to parallelise our script across the range 1 to 48. This creates 48 separate jobs each with a value between 1 and 48 stored in the variable <code>${SLURM_ARRAY_TASK_ID}</code>. Our <code>BET</code> call changes the subject number with this variable for each job.</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-camcan\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 5 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n#SBATCH --array=1-48\n\nmodule purge\nmodule load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n\nset -e\n\nbet subject${SLURM_ARRAY_TASK_ID}.nii.gz subject${SLURM_ARRAY_TASK_ID}_brain.nii.gz\n</code></pre> <p>Submitting this script to the cluster will run BET 48 times on each input from <code>subject1.nii.gz</code> to <code>subject48.nii.gz</code>.</p>"},{"location":"fsl/fsl/#fsl-in-a-container","title":"FSL in a container","text":"<p>Sometime we may want more control over software versions that are supported by pre-compiled BEAR App. We can install FSL within a controlled container using the following job script. This creates a container file <code>FSL.sif</code> from the NeuroDesk container specification.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 10 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n#SBATCH --constraint icelake\n\nset -e\n\napptainer pull --name FSL.sif docker://vnmd/fsl_6.0.4\n</code></pre> <p>We can sumbit this job to the cluster using <code>sbatch</code> as normal. Once the <code>FSL.sif</code> has been created we can run future cluster jobs through it.</p> <p>For example, this job script runs <code>fsl_anat</code> on a single dataset using our FSL container.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account quinna-camcan\n#SBATCH --qos bbdefault\n#SBATCH --time 60\n#SBATCH --nodes 1 # ensure the job runs on a single node\n#SBATCH --ntasks 5 # this will give you circa 40G RAM and will ensure faster conversion to the .sif format\n\nmodule purge\nmodule load bluebear\n\nset -e\n\napptainer exec FSL.sif  fsl_anat -i subject1.nii.gz -o subject1\n</code></pre> <p>This can be combined with array jobs from the example above to run many container-based analyses together in parallel.</p>"},{"location":"matlab/eeglab/","title":"EEGLab","text":"<p>Coming soon! please email andrew or raise a PR on github if you'd like to contribute to this page</p>"},{"location":"matlab/fieldtrip/","title":"Fieldtrip on Slurm","text":"<p>Note</p> <p>Example contributed by Ben Griffiths.*</p> <p>This is an example script running a fieldtrip analysis on EEG data acqurired during a visual flicker task.</p> <p>The data is read in, filtered, epoched, ICA'd, re-referenced, then plotted. The core function can be executed on the MatLab GUI App during an interactive session, or submitted to BlueEBAR using the bash script below.</p>"},{"location":"matlab/fieldtrip/#core-processing-script","title":"Core processing script","text":"<p>The following code can be saved as <code>basic_preprocessing.m</code>.</p> <pre><code>%% Basic Preprocessing\n% A script to demonstrate how one can (superficially) preprocessing EEG\n% data using Fieldtrip, Matlab and BlueBEAR.\n%\n% Benjamin J. Griffiths (b.griffiths.1 [at] bham.ac.uk)\n% 28th March 2023\n\n%% Prepare Workspace\n% define root directory where data is stored\nroot_dir = '/rds/projects/g/griffibz-example-project/msc-eeg-23/';\n\n% add fieldtrip to path\naddpath('/rds/projects/g/griffibz-example-project/fieldtrip/')\nft_defaults\n\n% define participant number\nsubj = 1;\n\n%% Filter Raw Data\n% load data\ncfg         = [];\ncfg.dataset = sprintf('%s/bids/sub-%02.0f/eeg/sub-%02.0f_task-eeg-flicker_eeg.eeg', root_dir, subj, subj); % dynamically determine dataset name\ndata        = ft_preprocessing(cfg);\n\n% remove external and trigger channels\ncfg         = [];\ncfg.channel = {'all', '-EX*', '-Status'}; % select all channels except any external (-EX*) or trigger (-Status) channel\ndata        = ft_selectdata(cfg, data);\n\n% filter data\ncfg             = [];\n%cfg.hpfilter    = 'yes';   % apply high-pass filter\n%cfg.hpfreq      = 0.8;     % use high-pass to suppress frequencies &lt; 0.8Hz\ncfg.lpfilter    = 'yes';   % apply low-pass filter\ncfg.lpfreq      = 120;     % use low-pass to suppress frequencies &gt; 120Hz\ncfg.bsfilter    = 'yes';   % apply band-pass filter\ncfg.bsfreq      = [49 51]; % use band-pass to suppress frequencies netween 49Hz and 51Hz\ndata            = ft_preprocessing(cfg, data);\n\n%% Epoch Data\n% load in BIDS event file\nevents = readtable(sprintf('%s/bids/sub-%02.0f/eeg/sub-%02.0f_task-eeg-flicker_events.tsv', root_dir, subj, subj),'Filetype','text'); % dynamically determine dataset name\n\n% define Fieldtrip-style event structure\ntrl_start = -2; % start trial 2 seconds before trigger\ntrl_end = 4; % end trial 4 seconds after trigger\ntrl_def(:,1) = events.sample + (trl_start * data.fsample); % define samples to start trial\ntrl_def(:,2) = events.sample + (trl_end * data.fsample); % define samples to end trial\ntrl_def(:,3) = trl_start * data.fsample; % define when time = 0 occurs relative to start of trial\n\n% epoch data\ncfg = [];\ncfg.trl = trl_def;\ndata = ft_redefinetrial(cfg, data);\n\n% load in trialinfo\nload(sprintf('%s/bids/sourcedata/sub-%02.0f_trialinfo.mat', root_dir, subj))\ndata.trialinfo = trialinfo; % add trialinfo to data structure\n\n% tidy workspace\nclear events trl_start trl_end trl_def trialinfo\n\n%% Run ICA\n% restrict to retrieval trials\ncfg         = [];\ncfg.trials  = find(cellfun(@(x) strcmpi(x.trl_type, 'retrieval'), data.trialinfo));\ndata        = ft_selectdata(cfg, data);\n\n% reduce sample rate\ncfg = [];\ncfg.resamplefs = 256; % drop sample rate from 1024Hz to 256Hz\ndata = ft_resampledata(cfg, data);\n\n% run ica\nrng(subj) % set random seed to ensure reproducible outputs every time the function is run\nica = ft_componentanalysis([], data); % \"cfg\" need not be defined if using default settings\n\n% visualise first 20 components (commented to stop execution when running via Slurm)\n%ft_topoplotIC(struct('component',1:20,'layout','biosemi128.lay'), ica)\n\n% remove components\ncfg             = [];\ncfg.component   = [1 3]; % 1 = eyeblink, 3 = saccade\ndata            = ft_rejectcomponent(cfg, ica);\n\n%% Re-reference Data\n% re-reference to the average of all channels\ncfg = [];\ncfg.reref = 'yes';\ncfg.refchannel = 'all';\ndata = ft_preprocessing(cfg, data);\n\n%% Plot Results\n% get timelocked average of data\ncfg = [];\ncfg.channel = 'A*'; % restrict to posterior quadrant of channels\ntml = ft_timelockanalysis(cfg, data);\n\n% baseline correct timelocked average\ncfg = [];\ncfg.baseline = [-0.25 -0.05]; % set baseline as -250ms to -50ms\ntml = ft_timelockbaseline(cfg, tml);\n\n% plot ERP\nh = figure;\nsubplot(2,1,1); hold on\nplot(tml.time, mean(tml.avg))\nxlim([-0.5 2.5])\nxline(0,'k--')\nyline(0,'k-')\nxlabel('Time (s)')\nylabel('Amplitude (uV)')\ntitle('Visual Evoked Potential')\n\n% cycle through trials\npow = cell(8, 1); % create empty cells for eight conditions\nfor trl = 1 : numel(data.trial)\n    condition = data.trialinfo{trl}.ret_freq; % determine flicker condition\n    channels_A = cellfun(@(x) strncmpi(x, 'A', 1), data.label); % identify posterior channels\n    signal = data.trial{trl}(channels_A, :); % extract signal over posterior channels\n    pow{condition}(end+1,:) = mean(abs(fft(signal')')); % compute FFT\nend\n\n% determine frequencies of FFT\nfreqs = linspace(0, data.fsample, size(pow{1},2));\n\n% plot FFT for each condition\nsubplot(2,1,2); hold on\nfor condition = 1 : numel(pow)\n    plot(freqs,mean(pow{condition}));\nend\nxlim([6, 42])\nylim([0, 700])\ntitle('Power Spectrum')\nxlabel('Frequency (Hz)')\nylabel('Power (arb. units)')\nlegend({'60Hz','40Hz','30Hz','24Hz','20Hz','17.1Hz','15Hz','Baseline'})\n\n% save figure in root directory\nsaveas(h, sprintf('%s/basic_preproc_output.jpg', root_dir))\n</code></pre>"},{"location":"matlab/fieldtrip/#cluster-submit-script","title":"Cluster submit script","text":"<p>The following can be saved as a shell script and submitted to the cluster using <code>sbatch</code>.</p> <pre><code>#!/bin/bash\n\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --time 1:0:0\n#SBATCH --qos bbdefault\n#SBATCH --mail-type ALL\n\nset -e\n\nmodule purge; module load bluebear\nmodule load MATLAB/2021b\n\nmatlab -nodisplay -r \"basic_preprocessing; exit;\"\n</code></pre>"},{"location":"matlab/matlab/","title":"MatLab","text":"<p>Interactive MatLab sessions run as a GUI App accessible from the Bear Portal. Please follow the information on the Bear Technical Docs to start up an interactive MatLab session.</p> <p>Some parallelisation is available through parfor loops within single MatLab but users looking for to run many individual matlab scripts in parallel are likely to want to use the Slurm job submissions. Examples of both are included below.</p>"},{"location":"matlab/matlab/#neuroimaging-toolboxes","title":"Neuroimaging toolboxes","text":"<p>Neuroimaging toolboxes can be added to the MatLab path on BlueBEAR in the normal way. Toolboxes can be downloaded from the developer and stored on an RDS space. These folders can be added to the path within a MatLab session using <code>addpath</code>.</p> <pre><code>addpath(genpath('/rds/q/quinna-example-project/code/fieldtrip'))\n</code></pre> <p>These pages include some specific examples using popular MatLab toolboxes</p> <ul> <li>Fieldtrip</li> <li>EEGLab</li> <li>SPM</li> </ul>"},{"location":"matlab/matlab/#parallel-for-loop","title":"Parallel for-loop","text":"<p>Info</p> <p>Example contributed by Dagmar Fraser</p> <p>Simple parallelisation of a for-loop can be performed using parfor. This functionality is provided by MatLab and enables faster processing of <code>for</code> loops simply by changing the syntax at the start to say <code>parfor</code> rather than <code>for</code>.</p> <p>The following Matlab code performs some matrix calculations on simulated data. The inclusion of a <code>parfor</code> loop means that the code can take advantage of computers with multiple CPUs to accelerate processing.</p> <pre><code>tic\nn = 200;\nA= 500;\na = zeros(1,n);\nparfor i = 1:n\n    a(i) = max(abs(eig(rand(A))));\nend\ntoc\n</code></pre> <p>Run this a few times replacing <code>parfor</code> with <code>for</code> to get an idea of the time difference.</p> <p>Note</p> <p>Make sure you specify the appropriate number of cores when starting the MatLab GUI App, you may not notice a substantial speed-up if you run MatLab using the default of 4 cores. Do try to avoid asking for substantially more than you might need however - BlueBEAR is a shared resource.</p>"},{"location":"matlab/matlab/#submitting-matlab-jobs-with-parfor-to-bear","title":"Submitting Matlab jobs with parfor to Bear","text":"<p>You can run this code in an interactive Matlab session, or save it as a script that can be executed on the big cluster. If we save the code in the previous example as <code>parforDemo.m</code>, we can write a second 'submission' script to execute it on the cluster.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 8\n#SBATCH --time 5:0\n#SBATCH --qos bbshort\n#SBATCH --mail-type ALL\n\nset -e\n\nmodule purge\nmodule load bluebear\nmodule load MATLAB/2020a\n\nmatlab -nodisplay -r parforDemo\n</code></pre> <p>If we save that second script as <code>RunMyCode.sh</code> it can be run using <code>sbatch RunMyCode.sh</code> on a terminal to send the job to the cluster. We can monitor the progress of the job using the 'Active Jobs' tab in BlueBEAR portal.</p> <p>The <code>ntasks</code> line specifies we are looking to use 8 cores. The last line contains the filename we are sending to MATLAB to execute</p>"},{"location":"matlab/matlab/#submitting-multiple-matlab-jobs","title":"Submitting multiple MatLab jobs","text":"<p>Info</p> <p>Example contributed by Katharina Deucker</p> <p>The previous example submits a single Matlab job that uses <code>parfor</code> BlueBEAR, for larger analyses we may want to parallelise jobs across entire matlab instances. This can be done by submitting MatLab jobs to BEAR using Slurm. The BEAR Technical Docs contain a simple example on submitting a matlab job to bear.</p> <p>For neuroimaging analyses, you'll generally need to organise your scripts so that each part that you want to parallelise runs from a single function that takes a single ID as an argument. Here is a specific example that runs a function <code>e1_fun_ICA</code> on each of 48 datasets.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --time 30:0\n#SBATCH --mem 50G\n#SBATCH --qos bbdefault\n#SBATCH --array=1-48\n\nset -eu\n\nmodule purge; module load bluebear\n\n# load the MATLAB version you need\nmodule load MATLAB/2019b\n\n# apply matlab script to each index in the array\n# (the MATLAB script is programmed such that the input ID is used as the subject ID)\nmatlab -nodisplay -r \"run /rds/homes/d/dueckerk/startup.m, e1_fun_ICA(${SLURM_ARRAY_TASK_ID}), quit\"\n</code></pre>"},{"location":"matlab/spm/","title":"SPM12","text":"<p>More coming soon! please email andrew or raise a PR on github if you'd like to contribute to this page</p>"},{"location":"matlab/spm/#running-first-level-analyses-with-parfor","title":"Running first-level analyses with parfor","text":"<p>Info</p> <p>Example contributed by Arkady Konovalov</p> <p>Simple parallelisation of a for-loop can be performed using parfor. This functionality is provided by MatLab and enables faster processing of <code>for</code> loops simply by changing the syntax at the start to say <code>parfor</code> rather than <code>for</code>.</p> <p>Here is an example function which makes use of <code>parfor</code> whilst computing GLMs using SPM.</p> <pre><code>function glm_level1(model)\n% This function takes a model structure as input and performs first-level\n% estimations in a General Linear Model (GLM) analysis for a set of subjects.\n\nsubjects = model.Subj;\n\n% FIRST LEVEL (individual) estimations\n% Get the number of subjects to be processed.\nN = size(subjects,2);\n\n% Iterate over each subject in \"subjects\" using parallel processing\nparfor i = 1:N\n\n        % Get the current subject ID from \"subjects\"\n        id = subjects(i);\n\n        % Get the corresponding BIDS (Brain Imaging Data Structure) ID and\n        % session information for the current subject.\n        BIDS_id = model.ids{id};\n        BIDS_sess = model.sess{id};\n\n        % Construct the path to the GLM folder for the current subject.\n        path = [model.glmfolder BIDS_id];\n\n        % Construct the path to the SPM.mat file for the current subject.\n        modelfile = [path '/SPM.mat'];\n\n        % Delete the existing SPM.mat file for the current subject (clean\n        % up previously done models)\n        delete(modelfile);\n\n        % Create a job structure for the current subject.\n        job = analysis_job_func(BIDS_id, BIDS_sess, model);\n\n        % Create an empty cell array to be used as inputs for the \"spm_jobman\" function.\n        inputs = cell(0,1);\n\n        % Set the SPM defaults to 'FMRI'.\n        spm('defaults', 'FMRI');\n\n        % Run the current job using the \"spm_jobman\" function.\n        spm_jobman('run', job, inputs{:});\n\nend\n\nend\n</code></pre> <p>Note</p> <p>Make sure you specify the appropriate number of cores when starting the MatLab GUI App, you may not notice a substantial speed-up if you run MatLab using the default of 4 cores. Do try to avoid asking for substantially more than you might need however - BlueBEAR is a shared resource.</p>"},{"location":"python/conda/","title":"Use Conda to Manage Your Own Python Environments","text":"<p>Managing software dependencies can be one of the most annoying things in scientific computing, especially when dealing with complex systems with multiple packages and libraries. This is where conda comes in - a package management system that simplifies the task of installing, configuring, and managing software packages and their dependencies.</p> <p>BEAR system does offer a default Python environment, but it must be loaded beforehand, and it does not allow you to install additional packages that are not already provided. After creating your own Conda environment, you don't have to load them every time. Submitting cluster jobs becomes straightforward with simple scripts, as demonstrated in this example:</p> <pre><code>#!/bin/bash\nset -e\n#SBATCH settings\npython your_script.py\n</code></pre> <p>!!! note BEAR do not currently recommend using conda for installing Python packages. This is due to possible performance issues when installing packages with conda. BEAR recommend that you get in touch with them prior via an IT-ticket to performing intensive analyses using conda environments.</p> <p>If this seems interesting to you, or you need to use a custom environment, let's get started!</p>"},{"location":"python/conda/#what-is-conda","title":"What is conda","text":"<p>Conda is a platform-agnostic package management system that can be used to install and manage software packages and their dependencies. It is designed to work with multiple programming languages, including Python, R, and others.</p> <p>Advantages of using conda:</p> <ul> <li>Build your own Python env when you don't have <code>sudo</code> access</li> <li>Conda simplifies managing software packages and dependencies</li> <li>Conda allows for isolated environments for different projects or applications</li> <li>Conda facilitates switching between different package versions</li> <li>Conda provides access to a vast range of pre-built packages and libraries.</li> </ul>"},{"location":"python/conda/#how-to-install-conda","title":"How to install conda","text":"<p>First, go to your home directory and type:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n# start installation\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Follow the instructions on the screen to complete the installation (You can just leave everything as default, it should be fine most of the time). After the installation is complete, you need to restart your kernel to load it, type:</p> <pre><code>exec bash\n</code></pre> <p>And you should see your terminal changed, for example from <code>[&lt;usr&gt;@bb-pg-login04 ~]$</code> to <code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$</code>. This means you have a Python environment called <code>base</code> that is now active. Now you can start using conda to manage your own Python environments!</p>"},{"location":"python/conda/#use-base-environment","title":"Use <code>base</code> environment","text":"<p>If you accepted the default setting when installing, the <code>base</code> environment is the default environment, Every time you logged in to BEAR this environment will be loaded automatically.</p> <p>You can install any packages you need in the <code>base</code> environment with <code>pip</code> or <code>conda</code> command. For example, to install <code>matplotlib</code> and <code>scipy</code>:</p> <pre><code>conda install -c conda-forge matplotlib=3.5.2 scipy\n# or\npip install matplotlib==3.5.2 scipy\n</code></pre> <p>If you have the requirements.txt from the projects you are working with:</p> <pre><code>pip install -r requirements.txt\n# or \nconda install --file requirements.txt\n</code></pre> <p>After installing the packages, you can check the list of packages installed in the <code>base</code> environment with:</p> <pre><code>conda list\n# or \npip list\n</code></pre> <p>Congratulations! Now you can start using your own environment to do anything you like! Simply type:</p> <pre><code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$ python your_script.py\n# or \n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ python # to start a python shell\n# or \n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ ipython # to start a ipython shell\n</code></pre>"},{"location":"python/conda/#crate-virtual-environments-with-conda","title":"Crate Virtual Environments with Conda","text":"<p>In most cases, the <code>base</code> environment should be enough. But if you are working on multiple projects, especially when you have deep learning projects or developing a toolbox that might be sensitive to specific environmental conditions, it is important to take measures to ensure consistency and prevent any problems that could arise from changes in the environment.</p> <p>Type the following command to create and enter a new environment:</p> <pre><code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$ conda create -n &lt;name&gt; python=3.10 ipykernel\n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ conda activate &lt;name&gt;\n# Now you will see your terminal became:\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$\n# install the packages using the commands provided above\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$ pip install -r requirements.txt\n# run your Python scripts\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$ python your_script.py\n\n# Go back to the base environment\n(&lt;name&gt;) [&lt;usr&gt;@bb-pg-login04 ~]$ conda deactivate\n(base) [&lt;usr&gt;@bb-pg-login04 ~]$ \n</code></pre>"},{"location":"python/conda/#run-your-job-on-the-cluster","title":"Run your job on the cluster","text":"<p>As I have indicated before, your bash script is very simple:</p> <pre><code>#!/bin/bash\n# run_python.sh\nset -e\n#SBATCH --account &lt;your account&gt;\n#other settings\n\npython your_script.py\n</code></pre> <p>Then choose your preferred environment, let's say an environment called <code>neuroimaging</code>, and type the following command:</p> <pre><code>(base) [&lt;usr&gt;@bb-pg-login04 ~]$ conda activate neuroimaging\n(neuroimaging) [&lt;usr&gt;@bb-pg-login04 ~]$ sbatch run_python.sh\n# check your output:\n(neuroimaging) [&lt;usr&gt;@bb-pg-login04 ~]$ tail -f slurm-&lt;id&gt;.out \nProcessing subj_id: CC222555:  24%|\u2588\u2588\u258e       | 153/651 [08:42&lt;28:37,  3.45\n</code></pre>"},{"location":"python/mne/","title":"MNE Python","text":"<p>MNE-Python is a Python package for analysing electrophysiology (MEG, EEG, sEEG, ECoG, NIRS, etc) data.</p>"},{"location":"python/mne/#mne-python-versions","title":"MNE-Python Versions","text":"<p>Bear Apps has several versions of MNE-Python as modules.</p>"},{"location":"python/mne/#bear-modules","title":"Bear Modules","text":"<p>The following bash loads mne version 1.3.1 and its dependencies - an equivalent is availiable for JupyterLab.</p> <pre><code>module load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\n</code></pre> <p>Note that MNE-Python depends on a number of other python applications that will be loaded automatically. The above code will also load numpy, scipy, numba, matplotlib, sklearn and many other packages that are needed by MNE into your environment.</p> <pre><code>import mne\nimport numpy as np\n</code></pre>"},{"location":"python/mne/#mne-in-a-virtual-environment","title":"MNE in a virtual environment","text":"<p>If you want to use a specific version of MNE-Python that isn't supported by BEAR, you can install it into a virtual environment. This bash script provides an example. We load Python 3.9.5, create an environment and install MNE into the environment using pip.</p> <pre><code>#!/bin/bash\n\nmodule purge;\nmodule load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\nmodule load IPython/7.25.0-GCCcore-10.3.0\n\nexport VENV_DIR=\"${HOME}/virtual-environments\"\nexport VENV_PATH=\"${VENV_DIR}/mne-example-${BB_CPU}\"\n\n# Create master dir if necessary\nmkdir -p ${VENV_DIR}\necho ${VENV_PATH}\n\n# Check if virtual environment exists and create it if not\nif [[ ! -d ${VENV_PATH} ]]; then\n    python3 -m venv --system-site-packages ${VENV_PATH}\nfi\n\n# Activate virtual environment\nsource ${VENV_PATH}/bin/activate\n\n# Any additional installations\npip install mne==1.1.0\n</code></pre> <p>As with other examples, this can be copied directly into the terminal or saved as an shell script that can be executed in a terminal.</p>"},{"location":"python/mne/#evoked-response-example","title":"Evoked response example","text":"<p>The following code is adapted from the MNE-Python overvew of MEG/EEG analysis tutorial. It will download a small example file and run a quick analysis.</p> <p>You can run this code directly in a Python session on BEAR within an activated MNE environment. The file will be saved into your RDS home directory (eg <code>/rds/homes/q/quinna</code>) unless specified otherwise - please change the <code>sample_data_folder</code> variable if you'd like to save this file elsewhere.</p> <pre><code>import numpy as np\nimport mne\n\n# This will download example data into your RDS home directory by default -\n# change the next line if you want to save the file elsewhere!\nsample_data_folder = '/rds/homes/q/quinna/mne-data/'\nsample_data_raw_file = (sample_data_folder / 'MEG' / 'sample' /\n                        'sample_audvis_filt-0-40_raw.fif')\nraw = mne.io.read_raw_fif(sample_data_raw_file)\n\nevents = mne.find_events(raw, stim_channel='STI 014')\n\nevent_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,\n              'visual/right': 4, 'smiley': 5, 'buttonpress': 32}\n\nreject_criteria = dict(mag=4000e-15,     # 4000 fT\n                       grad=4000e-13,    # 4000 fT/cm\n                       eeg=150e-6,       # 150 \u00b5V\n                       eog=250e-6)       # 250 \u00b5V\n\nepochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5,\n                    reject=reject_criteria, preload=True)\n\nconds_we_care_about = ['auditory/left', 'auditory/right',\n                       'visual/left', 'visual/right']\nepochs.equalize_event_counts(conds_we_care_about)  # this operates in-place\naud_epochs = epochs['auditory']\nvis_epochs = epochs['visual']\n\nvis_evoked = vis_epochs.average()\n\nfig = vis_evoked.plot_joint()\nfig[0].savefig('my-mne-evoked-example-grad.png')\nfig[1].savefig('my-mne-evoked-example-mag.png')\nfig[2].savefig('my-mne-evoked-example-eeg.png')\n</code></pre> <p>We can save this as 'mne_python_example.py` as our core analysis script. This can be execute from a terminal session in which the appropriate python environment has been loaded. For example, we could open a 'BlueBEAR GUI' session, open a new terminal, change directory to the location of our script and run the following code:</p> <pre><code>module load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\n\npython mne_python_example.py\n</code></pre> <p>At the end you should have some new figures created next to your script.</p> <p> </p>"},{"location":"python/mne/#mne-on-the-cluster-example","title":"MNE on the cluster example","text":"<p>Warning</p> <p>There is currently a bug with this example to do with the automatic file downloading when running on the cluster. Running a simliar example on your own data fromo RDS should work fine.</p> <p>Alternatively, we can run our evoked responses analysis on the cluster. For this we'll need a job submission script. Let's use the following shell code to load the MNE module from BEAR and run our code.</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n\nmodule purge; module load bluebear\n\nmodule load bear-apps/2022a\nmodule load MNE-Python/1.3.1-foss-2022a\n\npython mne_python_example.py\n</code></pre> <p>Note</p> <p>We could equally create or load a virtual environment with a customised Python set-up in our job script. Here we use the built in BEAR module as it is all we need for the case in hand.</p> <p>We can save the job script as <code>run_mne_python_example.sh</code> and submit it to the cluster using <code>sbatch</code></p> <pre><code>sbatch run_mne_python_example.sh\n</code></pre> <p>You can see the progress of the job using the 'Active Jobs' page on BEAR portal or by reading the log files.</p>"},{"location":"python/osl/","title":"OSL","text":"<p>Coming soon!</p>"},{"location":"python/python/","title":"Python","text":"<p>Python is a interactive programming language known for being flexible and (relatively) simple to use. A vast range of scientific applications have be built in and around Python. Some of the most common are:</p> <ul> <li>numpy: fundamental array computing in python</li> <li>scipy: fundamental algorithms in python</li> <li>pandas: manipulation and analysis of data tables</li> <li>scikit-learn: Efficient tools for machine learning</li> </ul> <p>and many, many more. Many python packages are distributed on PyPI.org</p>"},{"location":"python/python/#python-versions-on-bear","title":"Python Versions on Bear","text":"<p>Python versions up to 3.10 are supported as loadable modules on BEAR Apps. These can be loaded into a terminal session ready for use</p> <pre><code>module load bear-apps/2022a\nmodule load Python/3.10.4-GCCcore-11.3.0\n</code></pre> <p>This will be sufficient to run a pure Python script inside that terminal session. Frequently we'll want to load a wider range of modules to use in the script. There are several ways to go about this.</p> <p>We could load these modules one at a time, ensuring that any versions relating to Python, FOSS or GCCCore all match each other.</p> <pre><code>module load bear-apps/2022a\nmodule load scikit-learn/1.1.2-foss-2022a\n</code></pre> <p>Note</p> <p>Modules will load any relevant dependencies at the same time, so loading <code>scikit-learn</code> will also load the relevant python version into the session. It is best to trust the dependencies built into the <code>module load</code> system and only define the minimum necessary modules in your session.</p> <p>Or we can load a bundle of applications. The <code>Scipy-bundle</code> includes a bunch of packages including numpy, scipy and pandas.</p> <pre><code>module load bear-apps/2022a\nmodule load SciPy-bundle/2022.05-foss-2022a\n</code></pre> <p>iPython is a powerful python console that you can use for interactive sessions in the terminal.</p> <pre><code>module load bear-apps/2022a\nmodule load SciPy-bundle/2022.05-foss-2022a\nmodule load matplotlib/3.5.2-foss-2022a\nmodule load IPython/8.5.0-GCCcore-11.3.0\n</code></pre> <p>You can then start an iPython session from a terminal;</p> <pre><code>ipython --pylab=tk\n</code></pre> <p>and start running some python code using your loaded libraries</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.random.randn(10,)/2 + x\n\nplt.figure()\nplt.plot(x, y, 'o')\nplt.show()\n</code></pre> <p>you can also save some python code into a file and run it on the command line (this is very useful for running jobs on the cluster later...). If we save the code above into a file called <code>my_plot.py</code> - we can run it in the terminal using</p> <pre><code>python my_plot.py\n</code></pre>"},{"location":"python/python/#submitting-python-jobs-to-the-cluster","title":"Submitting Python jobs to the cluster","text":"<p>We need to prepare two things to run python jobs on the BlueBEAR cluster: we need an executable python script to run the analysis and a bash script to prepare an environment and actually run our code.</p> <p>Let's make a simple example. The following script creates and saves out a simple scatter plot of some random data.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.random.randn(10,)/2 + x\n\nplt.figure()\nplt.plot(x, y, 'o')\nplt.title('This ran on the cluster!')\nplt.xlabel('Variable 1')\nplt.ylabel('Variable 2')\nfor tag in ['top', 'right']:\n    plt.gca().spines[tag].set_visible(False)\nplt.grid(True)\nplt.savefig('my_cluster_figure.png')\n</code></pre> <p>We can save this script as <code>quick_python_plot.py</code>. Next, we need a bash/slurm script to submit and run our Python code.</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n\nmodule purge; module load bluebear\n\nmodule load bear-apps/2022a\nmodule load SciPy-bundle/2022.05-foss-2022a\nmodule load matplotlib/3.5.2-foss-2022a\n\npython quick_python_plot.py\n</code></pre> <p>We can save this as <code>submit_quick_python_plot.sh</code> in a directory next to our Python code (Remember to update the projecet on line 2 to a BEAR project that you can access!).</p> <pre><code>sbatch submit_quick_python_plot.sh\n</code></pre> <p>You can monitor the progress of your job in the active jobs tracker on BEAR portal. Once it has finished you should find a nice figure saved in your directory.</p> <p></p>"},{"location":"python/python/#jupyterlab","title":"JupyterLab","text":"<p>Interactive python notebooks are available to run as a JupyterLab GUI App through the Bear Portal. The pre-installed python modules can be loaded as modules in the notebook session.</p> <p>Only the pre-installed modules available in Bear Apps are installable in the JupyterLab GUI App.</p>"},{"location":"python/python/#virtual-environments","title":"Virtual Environments","text":"<p>More involved analyses may required dependecies or package versions that aren't available on BEAR Apps. The next optionn for these analysis is to use virtual environments as described on the BEAR technical docs.</p> <p>The following bash script (adapted from the main docs) loads the standard BEAR modules for MNE-Python, creates a virtual environment and then installs the EMD package with pip.</p> <pre><code>#!/bin/bash\nset -e\n\n# Load our core modules from BEAR\nmodule purge; module load bluebear\nmodule load bear-apps/2021b\nmodule load Python/3.9.6-GCCcore-11.2.0\n\n# Prepare path locations and name for virtual environment\nexport VENV_DIR=\"${HOME}/virtual-environments\"\nexport VENV_PATH=\"${VENV_DIR}/my-virtual-env-${BB_CPU}\"\n\n# Create a master venv directory if necessary\nmkdir -p ${VENV_DIR}\n\n# Check if virtual environment exists and create it if not\nif [[ ! -d ${VENV_PATH} ]]; then\n    python3 -m venv --system-site-packages ${VENV_PATH}\nfi\n\n# Activate the virtual environment\nsource ${VENV_PATH}/bin/activate\n\n# Perform any required pip installations. For reasons of consistency we would recommend\n# that you define the version of the Python module \u2013 this will also ensure that if the\n# module is already installed in the virtual environment it won't be modified.\npip install emd==0.5.4\n</code></pre> <p>You can save this into a shell script such as <code>init_myenv.sh</code> and run it using <code>source init_myenv.sh</code> to create the environment. You can now run <code>init_myenv.sh</code> when opening a new terminal to initialise an environment before running scripts or interactive sessions. The code above is all you need for this option, you can add or change the dependencies in the script as you need.</p>"},{"location":"python/python/#python-on-the-cluster","title":"Python on the cluster","text":"<p>You can also adapt the script to submit jobs to the cluster. For this, we'll need to add the appropriate <code>slurm</code> commands to the start of the script and add a line running our analysis to the end. That might look something like this:</p> <pre><code>#!/bin/bash\n#SBATCH --account quinna-example-project\n#SBATCH --qos bbdefault\n\nset -e\n\n# Load our core modules from BEAR\nmodule purge; module load bluebear\nmodule load bear-apps/2021b\nmodule load Python/3.9.6-GCCcore-11.2.0\n\n# Prepare path locations and name for virtual environment\nexport VENV_DIR=\"${HOME}/virtual-environments\"\nexport VENV_PATH=\"${VENV_DIR}/my-virtual-env-${BB_CPU}\"\n\n# Create a master venv directory if necessary\nmkdir -p ${VENV_DIR}\n\n# Check if virtual environment exists and create it if not\nif [[ ! -d ${VENV_PATH} ]]; then\n    python3 -m venv --system-site-packages ${VENV_PATH}\nfi\n\n# Activate the virtual environment\nsource ${VENV_PATH}/bin/activate\n\n# Perform any required pip installations. For reasons of consistency we would recommend\n# that you define the version of the Python module \u2013 this will also ensure that if the\n# module is already installed in the virtual environment it won't be modified.\npip install emd==0.5.4\n\n# Python script to be run.\npython emd_example.py\n</code></pre> <p>Note the additional <code>SBATCH</code> options at the start and the <code>python emd_example.py</code> at the end. We can save this script as 'submit_emd_example.sh`.</p> <p>We'll need a python Let's use this as an example. We can save the following script as <code>emd_example.py</code> on RDS.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport emd\n\n# Define and simulate a simple signal\npeak_freq = 15\nsample_rate = 256\nseconds = 10\nnoise_std = .4\nx = emd.simulate.ar_oscillator(peak_freq, sample_rate, seconds,\n                               noise_std=noise_std, random_seed=42, r=.96)[:, 0]\nx = x*1e-4\nt = np.linspace(0, seconds, seconds*sample_rate)\n\n# Run a mask sift\nimf = emd.sift.mask_sift(x, max_imfs=5)\n\nfig = plt.figure()\nemd.plotting.plot_imfs(imf[:sample_rate*3, :], fig=fig)\nfig.savefig('my-emd-example.png')\n</code></pre> <p>Now, we can submit our job to the cluster.</p> <pre><code>sbatch submit_emd_example.sh\n</code></pre> <p>You can monitor the progress of your job in the active jobs tracker on BEAR portal. Once it has finished you should find a nice new figure saved in your working directory.</p> <p></p>"}]}